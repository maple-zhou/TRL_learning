# TRLé«˜çº§ç‰¹æ€§ä¸æ‰©å±•æœºåˆ¶æ·±åº¦åˆ†æ

## ğŸš€ æ ¸å¿ƒæ‰©å±•æœºåˆ¶

### 1. **å¤šé€‚é…å™¨è®­ç»ƒç³»ç»Ÿ**

TRLæ”¯æŒåœ¨åŒä¸€ä¸ªåŸºæ¨¡å‹ä¸Šè®­ç»ƒå¤šä¸ªé€‚é…å™¨ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„ç‰¹æ€§ï¼š

```python
# åœ¨PPOTrainerä¸­çš„å®ç°
class PPOTrainer:
    def __init__(self, ..., peft_config=None):
        self.model_adapter_name = None
        self.ref_adapter_name = None
        
    @contextmanager
    def null_ref_context(self):
        """åˆ‡æ¢é€‚é…å™¨çŠ¶æ€"""
        if self.ref_adapter_name:
            self.model.policy.set_adapter(self.ref_adapter_name)
        yield
        if self.ref_adapter_name:
            self.model.policy.set_adapter(self.model_adapter_name or "default")
```

**æ‰©å±•ä»·å€¼**ï¼š
- ğŸ¯ **å¹¶è¡Œè®­ç»ƒ**: åŒæ—¶è®­ç»ƒå¤šä¸ªä¸“é—¨åŒ–é€‚é…å™¨
- ğŸ’¾ **å†…å­˜æ•ˆç‡**: å…±äº«åŸºæ¨¡å‹å‚æ•°
- ğŸ”„ **åŠ¨æ€åˆ‡æ¢**: è¿è¡Œæ—¶åˆ‡æ¢ä¸åŒç­–ç•¥

### 2. **åˆ†å¸ƒå¼è®­ç»ƒé›†æˆ**

#### DeepSpeedé›†æˆ
```python
# æ£€æµ‹åˆ†å¸ƒå¼ç¯å¢ƒ
if self.is_deepspeed_enabled:
    backup_deepspeed = self.deepspeed
    self.deepspeed = self.model
```

#### FSDPæ”¯æŒ
TRLå®Œå…¨é›†æˆäº†PyTorchçš„FSDP(Fully Sharded Data Parallel)ï¼š
- ğŸ“Š **å†…å­˜åˆ†ç‰‡**: æ¨¡å‹å‚æ•°åˆ†ç‰‡å­˜å‚¨
- ğŸ”„ **è‡ªåŠ¨åŒæ­¥**: è®­ç»ƒæ—¶è‡ªåŠ¨å‚æ•°åŒæ­¥  
- âš¡ **é«˜æ•ˆé€šä¿¡**: ä¼˜åŒ–çš„æ¢¯åº¦åŒæ­¥

### 3. **æ¿€æ´»æ£€æŸ¥ç‚¹(Activation Checkpointing)**

```python
# åœ¨models/activation_offloading.pyä¸­å®ç°
class ActivationOffloadingWrapper:
    def forward(self, *args, **kwargs):
        # æ¿€æ´»æ£€æŸ¥ç‚¹é€»è¾‘
        # ç‰ºç‰²è®¡ç®—æ¢å†…å­˜
```

**æŠ€æœ¯åŸç†**ï¼š
- ğŸ’¾ **å†…å­˜ä¼˜åŒ–**: é‡è®¡ç®—è€Œéå­˜å‚¨ä¸­é—´æ¿€æ´»
- ğŸ”„ **ç©ºé—´æ¢æ—¶é—´**: å¢åŠ 30%è®¡ç®—ï¼Œå‡å°‘80%å†…å­˜
- ğŸ¯ **æ™ºèƒ½é€‰æ‹©**: è‡ªåŠ¨é€‰æ‹©æ£€æŸ¥ç‚¹ä½ç½®

## ğŸ”Œ æ’ä»¶åŒ–è®¾è®¡

### 1. **å›è°ƒç³»ç»Ÿæ‰©å±•**

```python
# trainer/callbacks.py
class RichProgressCallback(TrainerCallback):
    """å¯Œæ–‡æœ¬è¿›åº¦æ¡å›è°ƒ"""
    
class SyncRefModelCallback(TrainerCallback):
    """å‚è€ƒæ¨¡å‹åŒæ­¥å›è°ƒ"""
    
class WinRateCallback(TrainerCallback):
    """èƒœç‡ç»Ÿè®¡å›è°ƒ"""
```

**æ‰©å±•æ¨¡å¼**ï¼š
- ğŸ­ **è§‚å¯Ÿè€…æ¨¡å¼**: è®­ç»ƒäº‹ä»¶çš„ç›‘å¬å’Œå¤„ç†
- ğŸ”§ **çƒ­æ’æ‹”**: åŠ¨æ€æ·»åŠ /ç§»é™¤å›è°ƒ
- ğŸ“Š **åŠŸèƒ½è§£è€¦**: å°†æ—¥å¿—ã€ç›‘æ§ç­‰åŠŸèƒ½æ¨¡å—åŒ–

### 2. **æ•°æ®å¤„ç†æ’ä»¶**

```python
# data_utils.pyä¸­çš„å¯æ‰©å±•å‡½æ•°
def apply_chat_template(dataset, tokenizer, **kwargs):
    """åº”ç”¨èŠå¤©æ¨¡æ¿"""
    
def pack_dataset(dataset, **kwargs):
    """æ•°æ®é›†æ‰“åŒ…ä¼˜åŒ–"""
    
def maybe_extract_prompt(example, **kwargs):
    """æ™ºèƒ½æå–æç¤º"""
```

**è®¾è®¡ç‰¹ç‚¹**ï¼š
- ğŸ”„ **ç®¡é“åŒ–**: æ•°æ®å¤„ç†ç®¡é“å¯ç»„åˆ
- ğŸ¯ **æ™ºèƒ½æ£€æµ‹**: è‡ªåŠ¨æ£€æµ‹æ•°æ®æ ¼å¼
- ğŸ”§ **æ˜“äºæ‰©å±•**: æ·»åŠ æ–°çš„å¤„ç†å‡½æ•°

## ğŸŒŸ å‰æ²¿ç®—æ³•é›†æˆ

### æ–°å…´è®­ç»ƒç®—æ³•
```python
# ä»__init__.pyå¯ä»¥çœ‹å‡ºæ”¯æŒçš„ç®—æ³•ï¼š
trainers = [
    "PPOTrainer",       # ç»å…¸PPO
    "DPOTrainer",       # ç›´æ¥åå¥½ä¼˜åŒ–
    "ORPOTrainer",      # å¥‡å¶åå¥½ä¼˜åŒ–
    "KTOTrainer",       # Kahneman-Tverskyä¼˜åŒ–
    "CPOTrainer",       # çº¦æŸç­–ç•¥ä¼˜åŒ–
    "AlignPropTrainer", # å¯¹é½ä¼ æ’­
    "OnlineDPOTrainer", # åœ¨çº¿DPO
    "RLOOTrainer",      # ç•™ä¸€æ³•ä¼˜åŒ–
]
```

**ç®—æ³•ç‰¹ç‚¹**ï¼š
- ğŸ¯ **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰ç®—æ³•å…±äº«ç›¸ä¼¼API
- ğŸš€ **å³æ’å³ç”¨**: è½»æ¾åˆ‡æ¢ä¸åŒç®—æ³•
- ğŸ“ˆ **æŒç»­æ›´æ–°**: é›†æˆæœ€æ–°ç ”ç©¶æˆæœ

### å¤šæ¨¡æ€æ”¯æŒ
```python
# DDPO for æ–‡ç”Ÿå›¾
"DDPOTrainer",           # Diffusionæ¨¡å‹PPO
"GRPOTrainer",           # è§†è§‰ç”ŸæˆPPO
```

## ğŸ­ ç”Ÿäº§çº§ç‰¹æ€§

### 1. **å†…å­˜ç®¡ç†**
```python
# æ™ºèƒ½åƒåœ¾å›æ”¶
if step % args.gradient_accumulation_steps == 0:
    gc.collect()
    torch.cuda.empty_cache()

# æ¿€æ´»offloading
@add_start_docstrings_to_model_forward(ACTIVATION_OFFLOADING_FORWARD_DOC)
def offload_activations(model):
    # æ¿€æ´»å¸è½½é€»è¾‘
```

### 2. **ç›‘æ§å’Œæ—¥å¿—**
```python
# é›†æˆWandBã€TensorBoardç­‰
from transformers.integrations import get_reporting_integration_callbacks

# ä¸°å¯Œçš„ç»Ÿè®¡ä¿¡æ¯
stats = {
    "ppo/loss/policy": policy_loss.mean(),
    "ppo/loss/value": value_loss.mean(), 
    "ppo/loss/total": loss.mean(),
    "ppo/policy/entropy": entropy.mean(),
    "ppo/policy/approxkl": kl.mean(),
    # ... æ›´å¤šæŒ‡æ ‡
}
```

### 3. **é”™è¯¯æ¢å¤å’Œå®¹é”™**
```python
# è®­ç»ƒä¸­æ–­æ¢å¤
def resume_from_checkpoint(self, checkpoint_dir):
    # æ¢å¤è®­ç»ƒçŠ¶æ€é€»è¾‘
    
# å¼‚å¸¸å¤„ç†å’Œé™çº§
try:
    # å°è¯•é«˜çº§åŠŸèƒ½
except Exception:
    # é™çº§åˆ°åŸºç¡€åŠŸèƒ½
```

## ğŸ¨ æ‰©å±•ç‚¹è¯†åˆ«

### 1. **æ–°ç®—æ³•æ·»åŠ **
```python
# æ­¥éª¤ï¼š
# 1. ç»§æ‰¿åŸºç±»
class MyTrainer(Trainer):
    pass

# 2. å®ç°é…ç½®ç±»  
class MyConfig(TrainerArguments):
    pass

# 3. æ³¨å†Œåˆ°__init__.py
_import_structure["trainer"].append("MyTrainer")
```

### 2. **è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°**
```python
# åœ¨rewards/ç›®å½•æ·»åŠ æ–°å¥–åŠ±
class CustomReward:
    def __call__(self, texts, model_outputs):
        # è‡ªå®šä¹‰å¥–åŠ±è®¡ç®—é€»è¾‘
        return rewards
```

### 3. **æ•°æ®æ ¼å¼æ‰©å±•**
```python
# åœ¨data_utils.pyæ·»åŠ æ–°æ ¼å¼å¤„ç†
def process_custom_format(dataset):
    # è‡ªå®šä¹‰æ•°æ®å¤„ç†é€»è¾‘
    return processed_dataset
```

## ğŸ’¡ æ¡†æ¶è®¾è®¡å“²å­¦

### 1. **ç»„åˆä¼˜äºç»§æ‰¿**
- é€šè¿‡ç»„åˆä¸åŒç»„ä»¶å®ç°åŠŸèƒ½
- é™ä½è€¦åˆåº¦ï¼Œæé«˜çµæ´»æ€§

### 2. **çº¦å®šä¼˜äºé…ç½®**
- æ™ºèƒ½é»˜è®¤å€¼å‡å°‘é…ç½®è´Ÿæ‹…
- çº¦å®šç»Ÿä¸€çš„æ¥å£å’Œå‘½å

### 3. **æ¸è¿›å¼å¤æ‚åº¦**
- ç®€å•åœºæ™¯å¼€ç®±å³ç”¨
- å¤æ‚éœ€æ±‚æ·±åº¦å®šåˆ¶
- ä¸“å®¶çº§å®Œå…¨æ§åˆ¶

TRLçš„æ‰©å±•æœºåˆ¶ä½“ç°äº†**ç°ä»£MLæ¡†æ¶**çš„è®¾è®¡ç²¾é«“ï¼šæ—¢ä¿æŒäº†ç®—æ³•çš„ç†è®ºæ­£ç¡®æ€§ï¼Œåˆæä¾›äº†å·¥ç¨‹å®è·µçš„çµæ´»æ€§ï¼