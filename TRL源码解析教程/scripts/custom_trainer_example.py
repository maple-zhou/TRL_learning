#!/usr/bin/env python3
"""
è‡ªå®šä¹‰è®­ç»ƒå™¨å®æˆ˜ç¤ºä¾‹
åŸºäºTRLæ¡†æ¶å¼€å‘ä¸€ä¸ªå¤šç›®æ ‡ä¼˜åŒ–çš„è‡ªå®šä¹‰è®­ç»ƒå™¨
"""

import torch
import torch.nn as nn
from typing import Optional, List, Dict, Any
from dataclasses import dataclass, field
from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM
from trl import PPOConfig, PPOTrainer
from datasets import Dataset
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict

@dataclass
class CustomRLConfig(PPOConfig):
    """è‡ªå®šä¹‰å¼ºåŒ–å­¦ä¹ é…ç½®ç±»"""
    
    # å¤šç›®æ ‡ä¼˜åŒ–å‚æ•°
    fluency_weight: float = field(default=0.4, metadata={"help": "æµç•…æ€§æƒé‡"})
    creativity_weight: float = field(default=0.3, metadata={"help": "åˆ›é€ æ€§æƒé‡"})
    relevance_weight: float = field(default=0.3, metadata={"help": "ç›¸å…³æ€§æƒé‡"})
    
    # åŠ¨æ€è°ƒæ•´å‚æ•°
    enable_dynamic_weights: bool = field(default=True, metadata={"help": "å¯ç”¨åŠ¨æ€æƒé‡è°ƒæ•´"})
    weight_adaptation_rate: float = field(default=0.01, metadata={"help": "æƒé‡é€‚åº”é€Ÿç‡"})
    
    # é«˜çº§ç›‘æ§å‚æ•°
    enable_rich_logging: bool = field(default=True, metadata={"help": "å¯ç”¨ä¸°å¯Œæ—¥å¿—"})
    diversity_penalty: float = field(default=0.1, metadata={"help": "å¤šæ ·æ€§æƒ©ç½šç³»æ•°"})
    
    # è‡ªå®šä¹‰é‡‡æ ·å‚æ•°
    custom_temperature_schedule: bool = field(default=True, metadata={"help": "è‡ªå®šä¹‰æ¸©åº¦è°ƒåº¦"})
    initial_temperature: float = field(default=1.0, metadata={"help": "åˆå§‹æ¸©åº¦"})
    final_temperature: float = field(default=0.1, metadata={"help": "æœ€ç»ˆæ¸©åº¦"})

class MultiObjectiveReward:
    """å¤šç›®æ ‡å¥–åŠ±å‡½æ•°"""
    
    def __init__(self, config: CustomRLConfig):
        self.config = config
        self.weights = {
            'fluency': config.fluency_weight,
            'creativity': config.creativity_weight,
            'relevance': config.relevance_weight
        }
        self.history = defaultdict(list)
    
    def compute_fluency_score(self, texts: List[str]) -> List[float]:
        """è®¡ç®—æµç•…æ€§å¾—åˆ†"""
        scores = []
        for text in texts:
            score = 0.5  # åŸºç¡€åˆ†
            
            # æ£€æŸ¥è¯­è¨€æµç•…æ€§æŒ‡æ ‡
            sentences = text.split('ã€‚')
            if len(sentences) > 1:
                # æœ‰å®Œæ•´å¥å­ç»“æ„
                score += 0.2
            
            # æ£€æŸ¥è¿æ¥è¯ä½¿ç”¨
            connectors = ['å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶è€Œ', 'å¹¶ä¸”', 'è€Œä¸”']
            connector_count = sum(1 for word in connectors if word in text)
            score += min(0.3, connector_count * 0.1)
            
            # é¿å…é‡å¤
            words = text.split()
            if len(words) > 0:
                unique_ratio = len(set(words)) / len(words)
                score += unique_ratio * 0.3
            
            scores.append(max(0, min(1, score)))
        
        return scores
    
    def compute_creativity_score(self, texts: List[str]) -> List[float]:
        """è®¡ç®—åˆ›é€ æ€§å¾—åˆ†"""
        scores = []
        creative_indicators = [
            'æƒ³è±¡', 'åˆ›æ–°', 'ç‹¬ç‰¹', 'æ–°é¢–', 'æœ‰è¶£', 'å¥‡å¦™',
            'creative', 'innovative', 'unique', 'novel'
        ]
        
        for text in texts:
            score = 0.3  # åŸºç¡€åˆ›é€ æ€§
            text_lower = text.lower()
            
            # åˆ›é€ æ€§è¯æ±‡
            for indicator in creative_indicators:
                if indicator in text_lower:
                    score += 0.2
            
            # è¯æ±‡å¤šæ ·æ€§
            words = text.split()
            if len(words) > 5:
                vocab_diversity = len(set(words)) / len(words)
                score += vocab_diversity * 0.3
            
            # é•¿åº¦é€‚ä¸­å¥–åŠ±
            if 30 <= len(text) <= 150:
                score += 0.2
            
            scores.append(max(0, min(1, score)))
        
        return scores
    
    def compute_relevance_score(self, prompts: List[str], texts: List[str]) -> List[float]:
        """è®¡ç®—ç›¸å…³æ€§å¾—åˆ†"""
        scores = []
        
        for prompt, text in zip(prompts, texts):
            score = 0.5
            
            # ç®€å•çš„å…³é”®è¯åŒ¹é…
            prompt_words = set(prompt.lower().split())
            text_words = set(text.lower().split())
            
            # è®¡ç®—äº¤é›†æ¯”ä¾‹
            if prompt_words:
                overlap = len(prompt_words & text_words) / len(prompt_words)
                score += overlap * 0.5
            
            scores.append(max(0, min(1, score)))
        
        return scores
    
    def compute_composite_reward(self, prompts: List[str], texts: List[str]) -> List[float]:
        """è®¡ç®—ç»¼åˆå¥–åŠ±"""
        fluency_scores = self.compute_fluency_score(texts)
        creativity_scores = self.compute_creativity_score(texts)
        relevance_scores = self.compute_relevance_score(prompts, texts)
        
        # è®°å½•å†å²ç”¨äºåŠ¨æ€è°ƒæ•´
        self.history['fluency'].extend(fluency_scores)
        self.history['creativity'].extend(creativity_scores)
        self.history['relevance'].extend(relevance_scores)
        
        # åŠ æƒå¹³å‡
        composite_scores = []
        for f, c, r in zip(fluency_scores, creativity_scores, relevance_scores):
            score = (
                f * self.weights['fluency'] +
                c * self.weights['creativity'] +
                r * self.weights['relevance']
            )
            composite_scores.append(score)
        
        return composite_scores
    
    def adapt_weights(self):
        """åŠ¨æ€è°ƒæ•´æƒé‡"""
        if not self.config.enable_dynamic_weights:
            return
        
        if len(self.history['fluency']) < 50:  # éœ€è¦è¶³å¤Ÿæ ·æœ¬
            return
        
        # è®¡ç®—å„ç»´åº¦çš„æœ€è¿‘è¡¨ç°
        recent_fluency = np.mean(self.history['fluency'][-20:])
        recent_creativity = np.mean(self.history['creativity'][-20:])
        recent_relevance = np.mean(self.history['relevance'][-20:])
        
        # å¦‚æœæŸä¸ªç»´åº¦è¡¨ç°ä¸å¥½ï¼Œå¢åŠ å…¶æƒé‡
        adaptation_rate = self.config.weight_adaptation_rate
        
        if recent_fluency < 0.6:
            self.weights['fluency'] += adaptation_rate
        if recent_creativity < 0.6:
            self.weights['creativity'] += adaptation_rate
        if recent_relevance < 0.6:
            self.weights['relevance'] += adaptation_rate
        
        # é‡æ–°å½’ä¸€åŒ–æƒé‡
        total_weight = sum(self.weights.values())
        for key in self.weights:
            self.weights[key] /= total_weight
        
        print(f"ğŸ”„ æƒé‡è°ƒæ•´: {self.weights}")

class CustomRLTrainer(PPOTrainer):
    """è‡ªå®šä¹‰å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, config: CustomRLConfig, **kwargs):
        # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
        super().__init__(config, **kwargs)
        
        # åˆå§‹åŒ–è‡ªå®šä¹‰ç»„ä»¶
        self.multi_objective_reward = MultiObjectiveReward(config)
        self.custom_config = config
        self.training_stats = defaultdict(list)
        self.step_count = 0
    
    def get_temperature_for_step(self, step: int) -> float:
        """è‡ªå®šä¹‰æ¸©åº¦è°ƒåº¦"""
        if not self.custom_config.custom_temperature_schedule:
            return 0.7  # é»˜è®¤æ¸©åº¦
        
        # çº¿æ€§è¡°å‡æ¸©åº¦è°ƒåº¦
        total_steps = self.custom_config.num_train_epochs * len(self.dataloader)
        progress = min(1.0, step / total_steps)
        
        initial_temp = self.custom_config.initial_temperature
        final_temp = self.custom_config.final_temperature
        
        current_temp = initial_temp * (1 - progress) + final_temp * progress
        return current_temp
    
    def custom_generate(self, query_tensors, **generation_kwargs):
        """è‡ªå®šä¹‰ç”Ÿæˆå‡½æ•°ï¼Œæ”¯æŒåŠ¨æ€æ¸©åº¦"""
        
        current_temp = self.get_temperature_for_step(self.step_count)
        generation_kwargs['temperature'] = current_temp
        
        # è°ƒç”¨çˆ¶ç±»ç”Ÿæˆæ–¹æ³•
        response_tensors = super().generate(
            query_tensors,
            **generation_kwargs
        )
        
        return response_tensors
    
    def compute_custom_rewards(self, query_tensors, response_tensors):
        """è®¡ç®—è‡ªå®šä¹‰å¥–åŠ±"""
        
        # è§£ç æ–‡æœ¬
        prompts = [self.tokenizer.decode(q, skip_special_tokens=True) for q in query_tensors]
        responses = [self.tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]
        
        # è®¡ç®—å¤šç›®æ ‡å¥–åŠ±
        rewards = self.multi_objective_reward.compute_composite_reward(prompts, responses)
        
        # åŠ¨æ€è°ƒæ•´æƒé‡
        if self.step_count % 10 == 0:  # æ¯10æ­¥è°ƒæ•´ä¸€æ¬¡
            self.multi_objective_reward.adapt_weights()
        
        return [torch.tensor(r) for r in rewards]
    
    def custom_step(self, queries, responses):
        """è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤"""
        
        # è®¡ç®—è‡ªå®šä¹‰å¥–åŠ±
        rewards = self.compute_custom_rewards(queries, responses)
        
        # æ‰§è¡ŒPPOæ›´æ–°
        stats = super().step(queries, responses, rewards)
        
        # è®°å½•è‡ªå®šä¹‰ç»Ÿè®¡ä¿¡æ¯
        if stats:
            self.training_stats['policy_loss'].append(stats.get('ppo/loss/policy', 0))
            self.training_stats['value_loss'].append(stats.get('ppo/loss/value', 0))
            self.training_stats['mean_reward'].append(np.mean([r.item() for r in rewards]))
            self.training_stats['current_weights'].append(self.multi_objective_reward.weights.copy())
        
        self.step_count += 1
        
        # æ¯20æ­¥æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        if self.step_count % 20 == 0:
            self.print_training_stats()
        
        return stats
    
    def print_training_stats(self):
        """æ‰“å°è®­ç»ƒç»Ÿè®¡ä¿¡æ¯"""
        if not self.training_stats['mean_reward']:
            return
        
        recent_reward = np.mean(self.training_stats['mean_reward'][-10:])
        recent_policy_loss = np.mean(self.training_stats['policy_loss'][-10:])
        
        print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡ (æ­¥éª¤ {self.step_count}):")
        print(f"   å¹³å‡å¥–åŠ±: {recent_reward:.3f}")
        print(f"   ç­–ç•¥æŸå¤±: {recent_policy_loss:.3f}")
        print(f"   å½“å‰æƒé‡: {self.multi_objective_reward.weights}")
    
    def visualize_training_progress(self):
        """å¯è§†åŒ–è®­ç»ƒè¿›åº¦"""
        if not self.training_stats['mean_reward']:
            print("âŒ æ²¡æœ‰è®­ç»ƒæ•°æ®å¯è§†åŒ–")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        
        # å¥–åŠ±æ›²çº¿
        axes[0, 0].plot(self.training_stats['mean_reward'])
        axes[0, 0].set_title('å¹³å‡å¥–åŠ±å˜åŒ–')
        axes[0, 0].set_xlabel('è®­ç»ƒæ­¥éª¤')
        axes[0, 0].set_ylabel('å¥–åŠ±')
        axes[0, 0].grid(True)
        
        # æŸå¤±æ›²çº¿
        axes[0, 1].plot(self.training_stats['policy_loss'], label='ç­–ç•¥æŸå¤±')
        axes[0, 1].plot(self.training_stats['value_loss'], label='ä»·å€¼æŸå¤±')
        axes[0, 1].set_title('æŸå¤±å‡½æ•°å˜åŒ–')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # æƒé‡å˜åŒ–
        if self.training_stats['current_weights']:
            weights_history = self.training_stats['current_weights']
            fluency_weights = [w['fluency'] for w in weights_history]
            creativity_weights = [w['creativity'] for w in weights_history]
            relevance_weights = [w['relevance'] for w in weights_history]
            
            axes[1, 0].plot(fluency_weights, label='æµç•…æ€§')
            axes[1, 0].plot(creativity_weights, label='åˆ›é€ æ€§')
            axes[1, 0].plot(relevance_weights, label='ç›¸å…³æ€§')
            axes[1, 0].set_title('æƒé‡åŠ¨æ€è°ƒæ•´')
            axes[1, 0].legend()
            axes[1, 0].grid(True)
        
        # å¥–åŠ±åˆ†å¸ƒ
        if len(self.training_stats['mean_reward']) > 20:
            axes[1, 1].hist(self.training_stats['mean_reward'][-50:], bins=20, alpha=0.7)
            axes[1, 1].set_title('æœ€è¿‘å¥–åŠ±åˆ†å¸ƒ')
            axes[1, 1].set_xlabel('å¥–åŠ±å€¼')
            axes[1, 1].set_ylabel('é¢‘æ¬¡')
        
        plt.tight_layout()
        plt.savefig('custom_training_progress.png', dpi=150)
        plt.show()
        
        print("ğŸ“ˆ è®­ç»ƒå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° custom_training_progress.png")

class AdaptiveKLController:
    """è‡ªé€‚åº”KLæ•£åº¦æ§åˆ¶å™¨"""
    
    def __init__(self, initial_kl_coef=0.05, target_kl=0.1):
        self.kl_coef = initial_kl_coef
        self.target_kl = target_kl
        self.kl_history = []
    
    def update(self, current_kl: float):
        """æ ¹æ®å½“å‰KLæ•£åº¦è°ƒæ•´ç³»æ•°"""
        self.kl_history.append(current_kl)
        
        # ä¿æŒæœ€è¿‘100ä¸ªKLå€¼
        if len(self.kl_history) > 100:
            self.kl_history = self.kl_history[-100:]
        
        # è®¡ç®—æœ€è¿‘çš„å¹³å‡KL
        recent_kl = np.mean(self.kl_history[-10:])
        
        # è‡ªé€‚åº”è°ƒæ•´KLç³»æ•°
        if recent_kl > self.target_kl * 1.5:
            # KLå¤ªå¤§ï¼Œå¢åŠ æƒ©ç½š
            self.kl_coef *= 1.1
        elif recent_kl < self.target_kl * 0.5:
            # KLå¤ªå°ï¼Œå‡å°‘æƒ©ç½š
            self.kl_coef *= 0.95
        
        # é™åˆ¶KLç³»æ•°èŒƒå›´
        self.kl_coef = max(0.001, min(1.0, self.kl_coef))
        
        return self.kl_coef

def create_demo_dataset():
    """åˆ›å»ºæ¼”ç¤ºæ•°æ®é›†"""
    prompts = [
        "è¯·å†™ä¸€ä¸ªåˆ›æ„æ•…äº‹ï¼š",
        "è§£é‡Šä¸€ä¸ªå¤æ‚æ¦‚å¿µï¼š",
        "ç»™å‡ºå®ç”¨å»ºè®®ï¼š",
        "æè¿°ä¸€ä¸ªåœºæ™¯ï¼š",
        "åˆ†æä¸€ä¸ªé—®é¢˜ï¼š",
    ] * 20
    
    return Dataset.from_dict({"query": prompts})

def demo_custom_trainer():
    """æ¼”ç¤ºè‡ªå®šä¹‰è®­ç»ƒå™¨"""
    
    print("ğŸ¯ è‡ªå®šä¹‰è®­ç»ƒå™¨æ¼”ç¤º")
    print("=" * 50)
    
    # 1. åˆ›å»ºé…ç½®
    config = CustomRLConfig(
        learning_rate=1e-5,
        batch_size=4,
        mini_batch_size=2,
        steps=30,
        fluency_weight=0.4,
        creativity_weight=0.3,
        relevance_weight=0.3,
        enable_dynamic_weights=True,
    )
    
    print(f"åˆå§‹æƒé‡é…ç½®:")
    print(f"  æµç•…æ€§: {config.fluency_weight}")
    print(f"  åˆ›é€ æ€§: {config.creativity_weight}")
    print(f"  ç›¸å…³æ€§: {config.relevance_weight}")
    
    # 2. å‡†å¤‡æ¨¡å‹å’Œæ•°æ®
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    from trl import AutoModelForCausalLMWithValueHead
    model = AutoModelForCausalLMWithValueHead.from_pretrained("gpt2")
    
    dataset = create_demo_dataset()
    
    # 3. åˆ›å»ºè‡ªå®šä¹‰è®­ç»ƒå™¨
    trainer = CustomRLTrainer(
        config=config,
        model=model,
        ref_model=None,
        tokenizer=tokenizer,
        dataset=dataset
    )
    
    print(f"\nğŸš€ å¼€å§‹è‡ªå®šä¹‰è®­ç»ƒ (å…±{config.steps}æ­¥)...")
    
    # 4. æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    for epoch, batch in enumerate(trainer.dataloader):
        if epoch >= config.steps:
            break
        
        print(f"\n--- æ­¥éª¤ {epoch + 1} ---")
        
        # ç”Ÿæˆå“åº”
        query_tensors = batch["input_ids"]
        current_temp = trainer.get_temperature_for_step(epoch)
        
        response_tensors = trainer.custom_generate(
            query_tensors,
            return_prompt=False,
            max_length=100,
            do_sample=True,
            temperature=current_temp,
            top_p=0.9
        )
        
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬
        if epoch < 3:  # åªæ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬
            query_text = tokenizer.decode(query_tensors[0], skip_special_tokens=True)
            response_text = tokenizer.decode(response_tensors[0], skip_special_tokens=True)
            print(f"æç¤º: {query_text}")
            print(f"å›å¤: {response_text}")
            print(f"æ¸©åº¦: {current_temp:.2f}")
        
        # è‡ªå®šä¹‰è®­ç»ƒæ­¥éª¤
        stats = trainer.custom_step(query_tensors, response_tensors)
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    
    # 5. å¯è§†åŒ–ç»“æœ
    trainer.visualize_training_progress()
    
    # 6. ä¿å­˜æ¨¡å‹
    save_choice = input("\næ˜¯å¦ä¿å­˜è®­ç»ƒåçš„æ¨¡å‹? (y/n): ")
    if save_choice.lower() == 'y':
        model.save_pretrained("./models/custom_rl_model")
        tokenizer.save_pretrained("./models/custom_rl_model")
        print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ° ./models/custom_rl_model")

def analyze_custom_implementation():
    """åˆ†æè‡ªå®šä¹‰å®ç°çš„è®¾è®¡æ¨¡å¼"""
    
    print("ğŸ” è‡ªå®šä¹‰è®­ç»ƒå™¨è®¾è®¡æ¨¡å¼åˆ†æ")
    print("=" * 50)
    
    patterns = [
        {
            "name": "ç­–ç•¥æ¨¡å¼",
            "description": "MultiObjectiveRewardæ”¯æŒä¸åŒçš„å¥–åŠ±è®¡ç®—ç­–ç•¥",
            "example": "å¯ä»¥è½»æ¾æ›¿æ¢ä¸ºå…¶ä»–å¥–åŠ±å‡½æ•°"
        },
        {
            "name": "è§‚å¯Ÿè€…æ¨¡å¼", 
            "description": "AdaptiveKLControllerç›‘æ§è®­ç»ƒçŠ¶æ€å¹¶è‡ªåŠ¨è°ƒæ•´",
            "example": "KLæ•£åº¦è¶…æ ‡æ—¶è‡ªåŠ¨è°ƒæ•´æƒ©ç½šç³»æ•°"
        },
        {
            "name": "æ¨¡æ¿æ–¹æ³•æ¨¡å¼",
            "description": "CustomRLTrainerç»§æ‰¿PPOTrainerï¼Œé‡å†™å…³é”®æ–¹æ³•",
            "example": "custom_step()æ–¹æ³•æ‰©å±•äº†æ ‡å‡†PPOæµç¨‹"
        },
        {
            "name": "é…ç½®æ¨¡å¼",
            "description": "CustomRLConfigæ‰©å±•äº†æ ‡å‡†é…ç½®",
            "example": "æ·»åŠ å¤šç›®æ ‡ä¼˜åŒ–çš„ä¸“ç”¨å‚æ•°"
        }
    ]
    
    for pattern in patterns:
        print(f"\nğŸ¨ {pattern['name']}:")
        print(f"   æè¿°: {pattern['description']}")
        print(f"   ç¤ºä¾‹: {pattern['example']}")

def main():
    print("ğŸ¯ TRLè‡ªå®šä¹‰è®­ç»ƒå™¨å¼€å‘å®æˆ˜")
    
    while True:
        print("\né€‰æ‹©æ“ä½œ:")
        print("1. è¿è¡Œè‡ªå®šä¹‰è®­ç»ƒå™¨æ¼”ç¤º")
        print("2. åˆ†æè®¾è®¡æ¨¡å¼")
        print("3. æŸ¥çœ‹è®­ç»ƒå¯è§†åŒ–")
        print("4. é€€å‡º")
        
        choice = input("è¯·é€‰æ‹© (1-4): ")
        
        if choice == "1":
            demo_custom_trainer()
        elif choice == "2":
            analyze_custom_implementation()
        elif choice == "3":
            # å¦‚æœæœ‰ä¿å­˜çš„å›¾ç‰‡ï¼Œæ˜¾ç¤ºè·¯å¾„
            import os
            if os.path.exists("custom_training_progress.png"):
                print("ğŸ“ˆ è®­ç»ƒå›¾è¡¨: custom_training_progress.png")
            else:
                print("âŒ è¯·å…ˆè¿è¡Œè®­ç»ƒæ¼”ç¤º")
        elif choice == "4":
            print("ğŸ‘‹ é€€å‡º")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main()