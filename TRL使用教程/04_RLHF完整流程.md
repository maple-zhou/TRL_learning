# RLHF (Reinforcement Learning from Human Feedback) 完整流程

## 1. RLHF三阶段训练

### 阶段一：监督微调 (SFT)
- 使用高质量对话数据微调预训练模型
- 让模型学会基本的对话格式
- 提高模型在目标任务上的表现

### 阶段二：奖励模型训练 (RM)
- 使用人类偏好数据训练奖励模型
- 奖励模型学会预测人类对回复的偏好
- 替代人类进行大规模评估

### 阶段三：PPO强化学习 (RL)
- 使用奖励模型的输出作为奖励信号
- 通过PPO算法优化策略模型
- 最大化期望奖励

## 2. 数据格式要求

### SFT数据格式
```json
{
  "instruction": "请解释什么是机器学习",
  "input": "",
  "output": "机器学习是一种人工智能技术..."
}
```

### 奖励模型数据格式
```json
{
  "instruction": "请解释什么是机器学习",
  "chosen": "机器学习是一种让计算机从数据中学习的技术...",
  "rejected": "机器学习就是电脑很聪明..."
}
```

## 3. 技术细节

### KL散度惩罚
- 防止模型偏离原始行为太远
- KL(π_new || π_ref) 计算新旧策略差异
- 通过β参数控制惩罚强度

### 价值函数估计
- 估计状态的长期价值
- 帮助计算优势函数
- 减少方差，稳定训练

### 经验重放
- 重复使用收集的经验
- 提高样本效率
- 但需要重要性采样校正