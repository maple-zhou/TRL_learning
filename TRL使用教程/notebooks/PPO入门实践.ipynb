{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO算法入门实践\n",
    "\n",
    "这个notebook将带你一步步了解和实践PPO算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"CUDA可用: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 理解PPO配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建PPO配置\n",
    "config = PPOConfig(\n",
    "    model_name=\"gpt2\",           # 基础模型名称\n",
    "    learning_rate=1.41e-5,       # 学习率\n",
    "    batch_size=8,               # 批大小\n",
    "    mini_batch_size=2,          # 小批大小\n",
    "    gradient_accumulation_steps=1, # 梯度累积\n",
    "    ppo_epochs=4,               # PPO训练轮数\n",
    "    cliprange=0.2,              # 剪切范围\n",
    "    vf_coef=0.1,                # 价值函数损失系数\n",
    "    seed=42,                    # 随机种子\n",
    "    steps=50,                   # 总训练步数\n",
    ")\n",
    "\n",
    "print(\"PPO配置创建完成\")\n",
    "print(f\"批大小: {config.batch_size}\")\n",
    "print(f\"学习率: {config.learning_rate}\")\n",
    "print(f\"训练步数: {config.steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 准备模型和数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载tokenizer和模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 设置特殊token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"模型加载完成\")\n",
    "print(f\"模型参数量: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建训练数据\n",
    "prompts = [\n",
    "    \"今天天气很好，\",\n",
    "    \"人工智能是\",\n",
    "    \"我最喜欢的颜色是\",\n",
    "    \"编程的乐趣在于\",\n",
    "    \"学习新知识让我\",\n",
    "] * 10  # 重复创建更多样本\n",
    "\n",
    "dataset = Dataset.from_dict({\"query\": prompts})\n",
    "print(f\"数据集大小: {len(dataset)}\")\n",
    "print(f\"示例提示: {dataset[0]['query']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 定义奖励函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(texts):\n",
    "    \"\"\"\n",
    "    奖励函数：鼓励积极、有意义的回复\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    # 积极词汇列表\n",
    "    positive_words = [\"好\", \"棒\", \"喜欢\", \"开心\", \"快乐\", \"有趣\", \"学习\", \"成长\"]\n",
    "    \n",
    "    for text in texts:\n",
    "        reward = 0.0\n",
    "        \n",
    "        # 基础奖励\n",
    "        reward += 0.1\n",
    "        \n",
    "        # 长度奖励 (适中长度)\n",
    "        if 10 <= len(text) <= 50:\n",
    "            reward += 0.5\n",
    "        \n",
    "        # 积极词汇奖励\n",
    "        for word in positive_words:\n",
    "            if word in text:\n",
    "                reward += 0.3\n",
    "        \n",
    "        # 避免重复\n",
    "        words = text.split()\n",
    "        if len(words) > len(set(words)) * 1.5:  # 重复词太多\n",
    "            reward -= 0.2\n",
    "        \n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# 测试奖励函数\n",
    "test_texts = [\"今天天气很好，我很开心\", \"重复重复重复重复\", \"学习让我快乐\"]\n",
    "test_rewards = reward_function(test_texts)\n",
    "\n",
    "for text, reward in zip(test_texts, test_rewards):\n",
    "    print(f\"文本: '{text}' -> 奖励: {reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 创建PPO训练器并开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建PPO训练器\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=config,\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset\n",
    ")\n",
    "\n",
    "print(\"PPO训练器创建完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练循环\n",
    "training_stats = []\n",
    "\n",
    "print(\"🚀 开始PPO训练...\")\n",
    "\n",
    "for epoch, batch in enumerate(ppo_trainer.dataloader):\n",
    "    if epoch >= 10:  # 只训练10步用于演示\n",
    "        break\n",
    "    \n",
    "    print(f\"\\n--- 训练步骤 {epoch + 1} ---\")\n",
    "    \n",
    "    # 生成回复\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        query_tensors,\n",
    "        return_prompt=False,\n",
    "        max_length=80,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    # 解码文本\n",
    "    batch_texts = []\n",
    "    for i in range(len(response_tensors)):\n",
    "        response_text = tokenizer.decode(response_tensors[i], skip_special_tokens=True)\n",
    "        batch_texts.append(response_text)\n",
    "        \n",
    "        # 显示第一个样本\n",
    "        if i == 0:\n",
    "            query_text = tokenizer.decode(query_tensors[i], skip_special_tokens=True)\n",
    "            print(f\"提示: {query_text}\")\n",
    "            print(f\"回复: {response_text}\")\n",
    "    \n",
    "    # 计算奖励\n",
    "    rewards = reward_function(batch_texts)\n",
    "    rewards = [torch.tensor(r) for r in rewards]\n",
    "    \n",
    "    print(f\"平均奖励: {np.mean([r.item() for r in rewards]):.3f}\")\n",
    "    \n",
    "    # PPO更新\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    \n",
    "    # 记录统计信息\n",
    "    if stats:\n",
    "        training_stats.append({\n",
    "            'epoch': epoch,\n",
    "            'mean_reward': np.mean([r.item() for r in rewards]),\n",
    "            'policy_loss': stats.get('ppo/loss/policy', 0),\n",
    "            'value_loss': stats.get('ppo/loss/value', 0)\n",
    "        })\n",
    "\n",
    "print(\"\\n🎉 训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 可视化训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练曲线\n",
    "if training_stats:\n",
    "    epochs = [s['epoch'] for s in training_stats]\n",
    "    rewards = [s['mean_reward'] for s in training_stats]\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, rewards, 'b-o')\n",
    "    plt.title('平均奖励随训练变化')\n",
    "    plt.xlabel('训练步骤')\n",
    "    plt.ylabel('平均奖励')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if 'policy_loss' in training_stats[0]:\n",
    "        policy_losses = [s['policy_loss'] for s in training_stats]\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, policy_losses, 'r-o')\n",
    "        plt.title('策略损失随训练变化')\n",
    "        plt.xlabel('训练步骤')\n",
    "        plt.ylabel('策略损失')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"📊 训练曲线绘制完成\")\n",
    "else:\n",
    "    print(\"❌ 没有训练统计数据\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 测试训练效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交互式测试\n",
    "def test_model(prompt):\n",
    "    \"\"\"测试模型生成\"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=100,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):].strip()\n",
    "\n",
    "# 测试几个例子\n",
    "test_prompts = [\n",
    "    \"今天是美好的一天，\",\n",
    "    \"学习人工智能让我\",\n",
    "    \"我的梦想是\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = test_model(prompt)\n",
    "    reward = reward_function([response])[0]\n",
    "    print(f\"\\n提示: {prompt}\")\n",
    "    print(f\"回复: {response}\")\n",
    "    print(f\"奖励: {reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 下一步学习\n",
    "\n",
    "完成这个基础示例后，你已经了解了：\n",
    "- PPO的基本工作流程\n",
    "- 如何定义奖励函数\n",
    "- 如何配置和运行PPO训练\n",
    "\n",
    "接下来我们将学习：\n",
    "1. RLHF完整训练流程\n",
    "2. DPO算法实现\n",
    "3. 自定义奖励模型\n",
    "4. 大规模训练技巧"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}