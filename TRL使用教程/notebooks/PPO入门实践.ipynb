{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPOç®—æ³•å…¥é—¨å®è·µ\n",
    "\n",
    "è¿™ä¸ªnotebookå°†å¸¦ä½ ä¸€æ­¥æ­¥äº†è§£å’Œå®è·µPPOç®—æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. å¯¼å…¥å¿…è¦çš„åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ç†è§£PPOé…ç½®å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºPPOé…ç½®\n",
    "config = PPOConfig(\n",
    "    model_name=\"gpt2\",           # åŸºç¡€æ¨¡å‹åç§°\n",
    "    learning_rate=1.41e-5,       # å­¦ä¹ ç‡\n",
    "    batch_size=8,               # æ‰¹å¤§å°\n",
    "    mini_batch_size=2,          # å°æ‰¹å¤§å°\n",
    "    gradient_accumulation_steps=1, # æ¢¯åº¦ç´¯ç§¯\n",
    "    ppo_epochs=4,               # PPOè®­ç»ƒè½®æ•°\n",
    "    cliprange=0.2,              # å‰ªåˆ‡èŒƒå›´\n",
    "    vf_coef=0.1,                # ä»·å€¼å‡½æ•°æŸå¤±ç³»æ•°\n",
    "    seed=42,                    # éšæœºç§å­\n",
    "    steps=50,                   # æ€»è®­ç»ƒæ­¥æ•°\n",
    ")\n",
    "\n",
    "print(\"PPOé…ç½®åˆ›å»ºå®Œæˆ\")\n",
    "print(f\"æ‰¹å¤§å°: {config.batch_size}\")\n",
    "print(f\"å­¦ä¹ ç‡: {config.learning_rate}\")\n",
    "print(f\"è®­ç»ƒæ­¥æ•°: {config.steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. å‡†å¤‡æ¨¡å‹å’Œæ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½tokenizerå’Œæ¨¡å‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")\n",
    "\n",
    "# è®¾ç½®ç‰¹æ®Štoken\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "print(f\"æ¨¡å‹å‚æ•°é‡: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºè®­ç»ƒæ•°æ®\n",
    "prompts = [\n",
    "    \"ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œ\",\n",
    "    \"äººå·¥æ™ºèƒ½æ˜¯\",\n",
    "    \"æˆ‘æœ€å–œæ¬¢çš„é¢œè‰²æ˜¯\",\n",
    "    \"ç¼–ç¨‹çš„ä¹è¶£åœ¨äº\",\n",
    "    \"å­¦ä¹ æ–°çŸ¥è¯†è®©æˆ‘\",\n",
    "] * 10  # é‡å¤åˆ›å»ºæ›´å¤šæ ·æœ¬\n",
    "\n",
    "dataset = Dataset.from_dict({\"query\": prompts})\n",
    "print(f\"æ•°æ®é›†å¤§å°: {len(dataset)}\")\n",
    "print(f\"ç¤ºä¾‹æç¤º: {dataset[0]['query']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å®šä¹‰å¥–åŠ±å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(texts):\n",
    "    \"\"\"\n",
    "    å¥–åŠ±å‡½æ•°ï¼šé¼“åŠ±ç§¯æã€æœ‰æ„ä¹‰çš„å›å¤\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    \n",
    "    # ç§¯æè¯æ±‡åˆ—è¡¨\n",
    "    positive_words = [\"å¥½\", \"æ£’\", \"å–œæ¬¢\", \"å¼€å¿ƒ\", \"å¿«ä¹\", \"æœ‰è¶£\", \"å­¦ä¹ \", \"æˆé•¿\"]\n",
    "    \n",
    "    for text in texts:\n",
    "        reward = 0.0\n",
    "        \n",
    "        # åŸºç¡€å¥–åŠ±\n",
    "        reward += 0.1\n",
    "        \n",
    "        # é•¿åº¦å¥–åŠ± (é€‚ä¸­é•¿åº¦)\n",
    "        if 10 <= len(text) <= 50:\n",
    "            reward += 0.5\n",
    "        \n",
    "        # ç§¯æè¯æ±‡å¥–åŠ±\n",
    "        for word in positive_words:\n",
    "            if word in text:\n",
    "                reward += 0.3\n",
    "        \n",
    "        # é¿å…é‡å¤\n",
    "        words = text.split()\n",
    "        if len(words) > len(set(words)) * 1.5:  # é‡å¤è¯å¤ªå¤š\n",
    "            reward -= 0.2\n",
    "        \n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# æµ‹è¯•å¥–åŠ±å‡½æ•°\n",
    "test_texts = [\"ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘å¾ˆå¼€å¿ƒ\", \"é‡å¤é‡å¤é‡å¤é‡å¤\", \"å­¦ä¹ è®©æˆ‘å¿«ä¹\"]\n",
    "test_rewards = reward_function(test_texts)\n",
    "\n",
    "for text, reward in zip(test_texts, test_rewards):\n",
    "    print(f\"æ–‡æœ¬: '{text}' -> å¥–åŠ±: {reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. åˆ›å»ºPPOè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºPPOè®­ç»ƒå™¨\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=config,\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset\n",
    ")\n",
    "\n",
    "print(\"PPOè®­ç»ƒå™¨åˆ›å»ºå®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒå¾ªç¯\n",
    "training_stats = []\n",
    "\n",
    "print(\"ğŸš€ å¼€å§‹PPOè®­ç»ƒ...\")\n",
    "\n",
    "for epoch, batch in enumerate(ppo_trainer.dataloader):\n",
    "    if epoch >= 10:  # åªè®­ç»ƒ10æ­¥ç”¨äºæ¼”ç¤º\n",
    "        break\n",
    "    \n",
    "    print(f\"\\n--- è®­ç»ƒæ­¥éª¤ {epoch + 1} ---\")\n",
    "    \n",
    "    # ç”Ÿæˆå›å¤\n",
    "    query_tensors = batch[\"input_ids\"]\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        query_tensors,\n",
    "        return_prompt=False,\n",
    "        max_length=80,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    # è§£ç æ–‡æœ¬\n",
    "    batch_texts = []\n",
    "    for i in range(len(response_tensors)):\n",
    "        response_text = tokenizer.decode(response_tensors[i], skip_special_tokens=True)\n",
    "        batch_texts.append(response_text)\n",
    "        \n",
    "        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬\n",
    "        if i == 0:\n",
    "            query_text = tokenizer.decode(query_tensors[i], skip_special_tokens=True)\n",
    "            print(f\"æç¤º: {query_text}\")\n",
    "            print(f\"å›å¤: {response_text}\")\n",
    "    \n",
    "    # è®¡ç®—å¥–åŠ±\n",
    "    rewards = reward_function(batch_texts)\n",
    "    rewards = [torch.tensor(r) for r in rewards]\n",
    "    \n",
    "    print(f\"å¹³å‡å¥–åŠ±: {np.mean([r.item() for r in rewards]):.3f}\")\n",
    "    \n",
    "    # PPOæ›´æ–°\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    \n",
    "    # è®°å½•ç»Ÿè®¡ä¿¡æ¯\n",
    "    if stats:\n",
    "        training_stats.append({\n",
    "            'epoch': epoch,\n",
    "            'mean_reward': np.mean([r.item() for r in rewards]),\n",
    "            'policy_loss': stats.get('ppo/loss/policy', 0),\n",
    "            'value_loss': stats.get('ppo/loss/value', 0)\n",
    "        })\n",
    "\n",
    "print(\"\\nğŸ‰ è®­ç»ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶è®­ç»ƒæ›²çº¿\n",
    "if training_stats:\n",
    "    epochs = [s['epoch'] for s in training_stats]\n",
    "    rewards = [s['mean_reward'] for s in training_stats]\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, rewards, 'b-o')\n",
    "    plt.title('å¹³å‡å¥–åŠ±éšè®­ç»ƒå˜åŒ–')\n",
    "    plt.xlabel('è®­ç»ƒæ­¥éª¤')\n",
    "    plt.ylabel('å¹³å‡å¥–åŠ±')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if 'policy_loss' in training_stats[0]:\n",
    "        policy_losses = [s['policy_loss'] for s in training_stats]\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, policy_losses, 'r-o')\n",
    "        plt.title('ç­–ç•¥æŸå¤±éšè®­ç»ƒå˜åŒ–')\n",
    "        plt.xlabel('è®­ç»ƒæ­¥éª¤')\n",
    "        plt.ylabel('ç­–ç•¥æŸå¤±')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ğŸ“Š è®­ç»ƒæ›²çº¿ç»˜åˆ¶å®Œæˆ\")\n",
    "else:\n",
    "    print(\"âŒ æ²¡æœ‰è®­ç»ƒç»Ÿè®¡æ•°æ®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æµ‹è¯•è®­ç»ƒæ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# äº¤äº’å¼æµ‹è¯•\n",
    "def test_model(prompt):\n",
    "    \"\"\"æµ‹è¯•æ¨¡å‹ç”Ÿæˆ\"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=100,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response[len(prompt):].strip()\n",
    "\n",
    "# æµ‹è¯•å‡ ä¸ªä¾‹å­\n",
    "test_prompts = [\n",
    "    \"ä»Šå¤©æ˜¯ç¾å¥½çš„ä¸€å¤©ï¼Œ\",\n",
    "    \"å­¦ä¹ äººå·¥æ™ºèƒ½è®©æˆ‘\",\n",
    "    \"æˆ‘çš„æ¢¦æƒ³æ˜¯\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = test_model(prompt)\n",
    "    reward = reward_function([response])[0]\n",
    "    print(f\"\\næç¤º: {prompt}\")\n",
    "    print(f\"å›å¤: {response}\")\n",
    "    print(f\"å¥–åŠ±: {reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ä¸‹ä¸€æ­¥å­¦ä¹ \n",
    "\n",
    "å®Œæˆè¿™ä¸ªåŸºç¡€ç¤ºä¾‹åï¼Œä½ å·²ç»äº†è§£äº†ï¼š\n",
    "- PPOçš„åŸºæœ¬å·¥ä½œæµç¨‹\n",
    "- å¦‚ä½•å®šä¹‰å¥–åŠ±å‡½æ•°\n",
    "- å¦‚ä½•é…ç½®å’Œè¿è¡ŒPPOè®­ç»ƒ\n",
    "\n",
    "æ¥ä¸‹æ¥æˆ‘ä»¬å°†å­¦ä¹ ï¼š\n",
    "1. RLHFå®Œæ•´è®­ç»ƒæµç¨‹\n",
    "2. DPOç®—æ³•å®ç°\n",
    "3. è‡ªå®šä¹‰å¥–åŠ±æ¨¡å‹\n",
    "4. å¤§è§„æ¨¡è®­ç»ƒæŠ€å·§"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}