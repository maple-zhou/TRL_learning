{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPO (Direct Preference Optimization) å®æˆ˜æ•™ç¨‹\n",
    "\n",
    "è¿™ä¸ªnotebookå°†å¸¦ä½ å­¦ä¹ DPOç®—æ³•ï¼Œä¸€ç§æ¯”RLHFæ›´ç®€å•é«˜æ•ˆçš„åå¥½ä¼˜åŒ–æ–¹æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DPOç®—æ³•æ ¸å¿ƒæ¦‚å¿µ\n",
    "\n",
    "### DPOçš„å…³é”®ä¼˜åŠ¿ï¼š\n",
    "- ğŸš€ **ç®€åŒ–æµç¨‹**: åªéœ€2æ­¥ (SFT â†’ DPO)ï¼Œè€Œä¸æ˜¯3æ­¥ (SFT â†’ RM â†’ PPO)\n",
    "- ğŸ“ˆ **è®­ç»ƒç¨³å®š**: é¿å…PPOçš„ä¸ç¨³å®šæ€§é—®é¢˜\n",
    "- ğŸ’° **é™ä½æˆæœ¬**: æ— éœ€è®­ç»ƒå¥–åŠ±æ¨¡å‹\n",
    "- ğŸ¯ **æ•ˆæœç›¸å½“**: åœ¨å¤šä¸ªåŸºå‡†ä¸Šä¸RLHFæ€§èƒ½ç›¸å½“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import DPOTrainer\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"ğŸ“š åº“å¯¼å…¥å®Œæˆ\")\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. å‡†å¤‡åå¥½æ•°æ®\n",
    "\n",
    "DPOéœ€è¦åå¥½å¯¹æ¯”æ•°æ®ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«ï¼š\n",
    "- `prompt`: ç”¨æˆ·æç¤º\n",
    "- `chosen`: æ›´å¥½çš„å›å¤\n",
    "- `rejected`: è¾ƒå·®çš„å›å¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preference_data():\n",
    "    \"\"\"åˆ›å»ºé«˜è´¨é‡çš„åå¥½å¯¹æ¯”æ•°æ®\"\"\"\n",
    "    \n",
    "    preference_pairs = [\n",
    "        {\n",
    "            \"prompt\": \"è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½\",\n",
    "            \"chosen\": \"äººå·¥æ™ºèƒ½(AI)æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿï¼Œå¦‚å­¦ä¹ ã€æ¨ç†ã€æ„ŸçŸ¥å’Œå†³ç­–ã€‚\",\n",
    "            \"rejected\": \"äººå·¥æ™ºèƒ½å°±æ˜¯äººå·¥çš„æ™ºèƒ½ï¼Œå¾ˆæ™ºèƒ½çš„äººå·¥ï¼Œæ™ºèƒ½äººå·¥ã€‚\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"ç»™æˆ‘ä¸€äº›å­¦ä¹ ç¼–ç¨‹çš„å»ºè®®\",\n",
    "            \"chosen\": \"å­¦ä¹ ç¼–ç¨‹å»ºè®®ï¼š1)é€‰æ‹©åˆé€‚çš„è¯­è¨€å¼€å§‹ï¼›2)å¤šåšå®é™…é¡¹ç›®ç»ƒä¹ ï¼›3)é˜…è¯»ä»–äººä¼˜ç§€ä»£ç ï¼›4)åŠ å…¥ç¼–ç¨‹ç¤¾åŒºäº¤æµï¼›5)ä¿æŒæŒç»­å­¦ä¹ çš„ä¹ æƒ¯ã€‚\",\n",
    "            \"rejected\": \"ç¼–ç¨‹å°±æ˜¯ç¼–ç¨‹ï¼Œå­¦å°±æ˜¯å­¦ï¼Œç¼–ç¨‹å­¦ä¹ å­¦ä¹ ç¼–ç¨‹ã€‚\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"æ¨èä¸€ä¸ªå‘¨æœ«æ´»åŠ¨\",\n",
    "            \"chosen\": \"æ¨èå»å…¬å›­æ•£æ­¥æˆ–éª‘è¡Œï¼Œæ—¢èƒ½é”»ç‚¼èº«ä½“åˆèƒ½äº«å—è‡ªç„¶é£å…‰ï¼Œå¦‚æœå¤©æ°”ä¸å¥½å¯ä»¥åœ¨å®¶è¯»ä¹¦æˆ–å­¦ä¹ æ–°æŠ€èƒ½ã€‚\",\n",
    "            \"rejected\": \"å‘¨æœ«å°±æ˜¯å‘¨æœ«æ´»åŠ¨æ´»åŠ¨æ´»åŠ¨æ´»åŠ¨ã€‚\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"å¦‚ä½•æé«˜å†™ä½œèƒ½åŠ›\",\n",
    "            \"chosen\": \"æé«˜å†™ä½œèƒ½åŠ›éœ€è¦ï¼šå¤šè¯»ä¼˜ç§€æ–‡ç« ç§¯ç´¯ç´ æï¼Œæ¯å¤©åšæŒå†™ä½œç»ƒä¹ ï¼Œå­¦ä¹ ä¸åŒçš„å†™ä½œæŠ€å·§ï¼Œè¯·ä»–äººç»™äºˆåé¦ˆå»ºè®®ã€‚\",\n",
    "            \"rejected\": \"å†™ä½œå†™ä½œå†™ä½œå†™ä½œå†™ä½œå†™ä½œå†™ä½œã€‚\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"æè¿°ä¸€ä¸‹ç†æƒ³çš„å‡æœŸ\",\n",
    "            \"chosen\": \"ç†æƒ³çš„å‡æœŸæ˜¯å’Œäº²æœ‹å¥½å‹ä¸€èµ·ï¼Œå»ä¸€ä¸ªé£æ™¯ä¼˜ç¾çš„åœ°æ–¹ï¼Œæ”¾æ…¢èŠ‚å¥ï¼Œäº«å—å½“ä¸‹ï¼Œå……ç”µä¼‘æ¯åä»¥æ›´å¥½çš„çŠ¶æ€æŠ•å…¥å·¥ä½œã€‚\",\n",
    "            \"rejected\": \"å‡æœŸå‡æœŸå‡æœŸå‡æœŸå‡æœŸå‡æœŸå‡æœŸã€‚\"\n",
    "        }\n",
    "    ] * 15  # åˆ›å»ºæ›´å¤šæ ·æœ¬\n",
    "    \n",
    "    return preference_pairs\n",
    "\n",
    "# åˆ›å»ºæ•°æ®é›†\n",
    "preference_data = create_preference_data()\n",
    "print(f\"åå¥½æ•°æ®é›†å¤§å°: {len(preference_data)}\")\n",
    "print(f\"\\nç¤ºä¾‹æ•°æ®:\")\n",
    "print(f\"æç¤º: {preference_data[0]['prompt']}\")\n",
    "print(f\"å¥½å›å¤: {preference_data[0]['chosen'][:50]}...\")\n",
    "print(f\"å·®å›å¤: {preference_data[0]['rejected'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. è®¾ç½®DPOè®­ç»ƒå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ¨¡å‹å’Œtokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# è®¾ç½®pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"æ¨¡å‹åŠ è½½å®Œæˆ: {model_name}\")\n",
    "print(f\"è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡†å¤‡æ•°æ®é›†\n",
    "train_dataset = Dataset.from_list(preference_data)\n",
    "\n",
    "print(f\"è®­ç»ƒæ•°æ®é›†åˆ›å»ºå®Œæˆï¼Œå¤§å°: {len(train_dataset)}\")\n",
    "print(\"\\næ•°æ®é›†å­—æ®µ:\", train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒå‚æ•°è®¾ç½®\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/dpo_gpt2\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,  # å°æ‰¹æ¬¡é¿å…å†…å­˜é—®é¢˜\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    logging_steps=5,\n",
    "    save_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    report_to=None,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_drop_last=True,\n",
    ")\n",
    "\n",
    "print(\"è®­ç»ƒå‚æ•°è®¾ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. å¼€å§‹DPOè®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºDPOè®­ç»ƒå™¨\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,  # å°†è‡ªåŠ¨åˆ›å»ºå‚è€ƒæ¨¡å‹\n",
    "    args=training_args,\n",
    "    beta=0.1,  # DPOæ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ä¸å‚è€ƒæ¨¡å‹çš„åå·®\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=128,\n",
    "    max_prompt_length=64,\n",
    ")\n",
    "\n",
    "print(\"âœ… DPOè®­ç»ƒå™¨åˆ›å»ºæˆåŠŸ\")\n",
    "print(f\"Betaå‚æ•°: {dpo_trainer.beta}\")\n",
    "print(f\"æœ€å¤§é•¿åº¦: {dpo_trainer.max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "print(\"ğŸš€ å¼€å§‹DPOè®­ç»ƒ...\")\n",
    "\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "dpo_trainer.train()\n",
    "\n",
    "print(\"âœ… DPOè®­ç»ƒå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ä¿å­˜å’Œæµ‹è¯•æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¨¡å‹\n",
    "dpo_trainer.save_model()\n",
    "tokenizer.save_pretrained(\"./models/dpo_gpt2\")\n",
    "\n",
    "print(\"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ° ./models/dpo_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹\n",
    "def generate_with_model(model, tokenizer, prompt, max_length=100):\n",
    "    \"\"\"ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬\"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# æµ‹è¯•å‡ ä¸ªä¾‹å­\n",
    "test_prompts = [\n",
    "    \"è¯·è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½\",\n",
    "    \"ç»™æˆ‘ä¸€äº›å­¦ä¹ ç¼–ç¨‹çš„å»ºè®®\", \n",
    "    \"å¦‚ä½•æé«˜å†™ä½œèƒ½åŠ›\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª æµ‹è¯•DPOè®­ç»ƒåçš„æ¨¡å‹:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = generate_with_model(model, tokenizer, prompt)\n",
    "    generated_part = response[len(prompt):].strip()\n",
    "    \n",
    "    print(f\"\\nğŸ“ æç¤º: {prompt}\")\n",
    "    print(f\"ğŸ¤– å›å¤: {generated_part}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ·±å…¥ç†è§£DPO\n",
    "\n",
    "### DPOæŸå¤±å‡½æ•°å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–DPOæŸå¤±å‡½æ•°\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def dpo_loss(logits_diff, beta=0.1):\n",
    "    \"\"\"DPOæŸå¤±å‡½æ•°\"\"\"\n",
    "    return -torch.log(torch.sigmoid(beta * logits_diff))\n",
    "\n",
    "# åˆ›å»ºæ•°æ®ç‚¹\n",
    "x = np.linspace(-5, 5, 100)\n",
    "betas = [0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for beta in betas:\n",
    "    y = [-np.log(1/(1 + np.exp(-beta * xi))) for xi in x]\n",
    "    plt.plot(x, y, label=f'Î²={beta}')\n",
    "\n",
    "plt.xlabel('log Ï€(chosen) - log Ï€(rejected)')\n",
    "plt.ylabel('DPO Loss')\n",
    "plt.title('DPOæŸå¤±å‡½æ•° vs Î²å‚æ•°')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š DPOæŸå¤±å‡½æ•°å¯è§†åŒ–å®Œæˆ\")\n",
    "print(\"ğŸ’¡ è§£è¯»: Î²å€¼è¶Šå¤§ï¼Œå¯¹åå¥½å·®å¼‚è¶Šæ•æ„Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. æ¨¡å‹å¯¹æ¯”å®éªŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¯”è¾ƒåŸå§‹æ¨¡å‹å’ŒDPOæ¨¡å‹\n",
    "def compare_models():\n",
    "    \"\"\"æ¯”è¾ƒåŸå§‹æ¨¡å‹å’ŒDPOè®­ç»ƒåæ¨¡å‹çš„è¾“å‡ºè´¨é‡\"\"\"\n",
    "    \n",
    "    # åŠ è½½åŸå§‹æ¨¡å‹\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "    original_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    if original_tokenizer.pad_token is None:\n",
    "        original_tokenizer.pad_token = original_tokenizer.eos_token\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"ç»™æˆ‘ä¸€ä¸ªå¥åº·ç”Ÿæ´»çš„å»ºè®®\",\n",
    "        \"è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ \",\n",
    "        \"æ¨èä¸€æœ¬å¥½ä¹¦\"\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ” æ¨¡å‹å¯¹æ¯”å®éªŒ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"\\nğŸ¯ æµ‹è¯• {i+1}: {prompt}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # åŸå§‹æ¨¡å‹è¾“å‡º\n",
    "        original_output = generate_with_model(original_model, original_tokenizer, prompt)\n",
    "        original_response = original_output[len(prompt):].strip()\n",
    "        \n",
    "        # DPOæ¨¡å‹è¾“å‡º\n",
    "        dpo_output = generate_with_model(model, tokenizer, prompt)\n",
    "        dpo_response = dpo_output[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"ğŸ“– åŸå§‹GPT2: {original_response}\")\n",
    "        print(f\"ğŸ¯ DPOè®­ç»ƒå: {dpo_response}\")\n",
    "\n",
    "# è¿è¡Œå¯¹æ¯”\n",
    "compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. DPO vs PPO æ€§èƒ½åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æDPOçš„ä¼˜åŠ¿\n",
    "comparison_data = {\n",
    "    'è®­ç»ƒé˜¶æ®µ': ['PPO (RLHF)', 'DPO'],\n",
    "    'æ­¥éª¤æ•°': [3, 2],\n",
    "    'éœ€è¦å¥–åŠ±æ¨¡å‹': ['æ˜¯', 'å¦'],\n",
    "    'è®­ç»ƒç¨³å®šæ€§': ['ä¸­ç­‰', 'é«˜'],\n",
    "    'å®ç°å¤æ‚åº¦': ['é«˜', 'ä½'],\n",
    "    'è®¡ç®—å¼€é”€': ['é«˜', 'ä¸­ç­‰']\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"ğŸ“Š DPO vs PPO å¯¹æ¯”è¡¨:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# ç»˜åˆ¶å¯¹æ¯”å›¾\n",
    "metrics = ['ç®€åŒ–ç¨‹åº¦', 'ç¨³å®šæ€§', 'æ•ˆç‡', 'æ˜“ç”¨æ€§']\n",
    "ppo_scores = [6, 7, 6, 5]\n",
    "dpo_scores = [9, 9, 8, 9]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(x - width/2, ppo_scores, width, label='PPO (RLHF)', alpha=0.8)\n",
    "ax.bar(x + width/2, dpo_scores, width, label='DPO', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('è¯„åˆ†')\n",
    "ax.set_title('DPO vs PPO ç»¼åˆè¯„ä¼°')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“ˆ DPOåœ¨å¤šä¸ªç»´åº¦ä¸Šéƒ½ä¼˜äºä¼ ç»ŸRLHF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. äº¤äº’å¼æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# äº¤äº’å¼æµ‹è¯•DPOæ¨¡å‹\n",
    "def interactive_test():\n",
    "    \"\"\"äº¤äº’å¼æµ‹è¯•DPOæ¨¡å‹\"\"\"\n",
    "    \n",
    "    print(\"ğŸ® DPOæ¨¡å‹äº¤äº’æµ‹è¯•\")\n",
    "    print(\"è¾“å…¥ä½ çš„é—®é¢˜ï¼Œæ¨¡å‹ä¼šç»™å‡ºå›ç­”\")\n",
    "    print(\"(åœ¨notebookä¸­è¿è¡Œæ­¤ä»£ç éœ€è¦äº¤äº’ç¯å¢ƒ)\")\n",
    "    \n",
    "    # è¿™é‡Œæä¾›å‡ ä¸ªæµ‹è¯•æ ·ä¾‹\n",
    "    test_questions = [\n",
    "        \"å¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ \",\n",
    "        \"æ¨èä¸€ä¸ªç¼–ç¨‹é¡¹ç›®\",\n",
    "        \"ç»™æˆ‘ä¸€äº›æ—¶é—´ç®¡ç†å»ºè®®\"\n",
    "    ]\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"\\nâ“ é—®é¢˜: {question}\")\n",
    "        response = generate_with_model(model, tokenizer, question)\n",
    "        answer = response[len(question):].strip()\n",
    "        print(f\"ğŸ¤– DPOæ¨¡å‹å›ç­”: {answer}\")\n",
    "\n",
    "# è¿è¡Œäº¤äº’æµ‹è¯•\n",
    "interactive_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. æ€»ç»“å’Œä¸‹ä¸€æ­¥\n",
    "\n",
    "é€šè¿‡è¿™ä¸ªæ•™ç¨‹ï¼Œä½ å·²ç»å­¦ä¼šäº†ï¼š\n",
    "\n",
    "âœ… **DPOçš„æ ¸å¿ƒæ¦‚å¿µ**ï¼šç›´æ¥åå¥½ä¼˜åŒ–ï¼Œæ— éœ€å¥–åŠ±æ¨¡å‹  \n",
    "âœ… **æ•°æ®å‡†å¤‡**ï¼šå¦‚ä½•æ„å»ºåå¥½å¯¹æ¯”æ•°æ®é›†  \n",
    "âœ… **æ¨¡å‹è®­ç»ƒ**ï¼šä½¿ç”¨TRLè¿›è¡ŒDPOè®­ç»ƒ  \n",
    "âœ… **æ•ˆæœè¯„ä¼°**ï¼šæ¯”è¾ƒè®­ç»ƒå‰åçš„æ¨¡å‹è¡¨ç°  \n",
    "\n",
    "### ä¸‹ä¸€æ­¥å­¦ä¹ æ–¹å‘ï¼š\n",
    "1. ğŸ”§ **é«˜çº§å®šåˆ¶**ï¼šè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°å’Œè®­ç»ƒç­–ç•¥\n",
    "2. ğŸ­ **ç”Ÿäº§éƒ¨ç½²**ï¼šæ¨¡å‹ä¼˜åŒ–å’Œéƒ¨ç½²æœ€ä½³å®è·µ\n",
    "3. ğŸ“Š **è¯„ä¼°æŒ‡æ ‡**ï¼šæ›´å…¨é¢çš„æ¨¡å‹è¯„ä¼°æ–¹æ³•\n",
    "4. ğŸ¯ **å®é™…åº”ç”¨**ï¼šå°†DPOåº”ç”¨åˆ°å…·ä½“ä»»åŠ¡ä¸­"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}