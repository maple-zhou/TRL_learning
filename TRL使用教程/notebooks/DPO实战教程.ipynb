{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DPO (Direct Preference Optimization) 实战教程\n",
    "\n",
    "这个notebook将带你学习DPO算法，一种比RLHF更简单高效的偏好优化方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DPO算法核心概念\n",
    "\n",
    "### DPO的关键优势：\n",
    "- 🚀 **简化流程**: 只需2步 (SFT → DPO)，而不是3步 (SFT → RM → PPO)\n",
    "- 📈 **训练稳定**: 避免PPO的不稳定性问题\n",
    "- 💰 **降低成本**: 无需训练奖励模型\n",
    "- 🎯 **效果相当**: 在多个基准上与RLHF性能相当"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import DPOTrainer\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"📚 库导入完成\")\n",
    "print(f\"PyTorch版本: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 准备偏好数据\n",
    "\n",
    "DPO需要偏好对比数据，每个样本包含：\n",
    "- `prompt`: 用户提示\n",
    "- `chosen`: 更好的回复\n",
    "- `rejected`: 较差的回复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preference_data():\n",
    "    \"\"\"创建高质量的偏好对比数据\"\"\"\n",
    "    \n",
    "    preference_pairs = [\n",
    "        {\n",
    "            \"prompt\": \"请解释什么是人工智能\",\n",
    "            \"chosen\": \"人工智能(AI)是计算机科学的一个分支，旨在创建能够执行通常需要人类智能的任务的系统，如学习、推理、感知和决策。\",\n",
    "            \"rejected\": \"人工智能就是人工的智能，很智能的人工，智能人工。\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"给我一些学习编程的建议\",\n",
    "            \"chosen\": \"学习编程建议：1)选择合适的语言开始；2)多做实际项目练习；3)阅读他人优秀代码；4)加入编程社区交流；5)保持持续学习的习惯。\",\n",
    "            \"rejected\": \"编程就是编程，学就是学，编程学习学习编程。\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"推荐一个周末活动\",\n",
    "            \"chosen\": \"推荐去公园散步或骑行，既能锻炼身体又能享受自然风光，如果天气不好可以在家读书或学习新技能。\",\n",
    "            \"rejected\": \"周末就是周末活动活动活动活动。\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"如何提高写作能力\",\n",
    "            \"chosen\": \"提高写作能力需要：多读优秀文章积累素材，每天坚持写作练习，学习不同的写作技巧，请他人给予反馈建议。\",\n",
    "            \"rejected\": \"写作写作写作写作写作写作写作。\"\n",
    "        },\n",
    "        {\n",
    "            \"prompt\": \"描述一下理想的假期\",\n",
    "            \"chosen\": \"理想的假期是和亲朋好友一起，去一个风景优美的地方，放慢节奏，享受当下，充电休息后以更好的状态投入工作。\",\n",
    "            \"rejected\": \"假期假期假期假期假期假期假期。\"\n",
    "        }\n",
    "    ] * 15  # 创建更多样本\n",
    "    \n",
    "    return preference_pairs\n",
    "\n",
    "# 创建数据集\n",
    "preference_data = create_preference_data()\n",
    "print(f\"偏好数据集大小: {len(preference_data)}\")\n",
    "print(f\"\\n示例数据:\")\n",
    "print(f\"提示: {preference_data[0]['prompt']}\")\n",
    "print(f\"好回复: {preference_data[0]['chosen'][:50]}...\")\n",
    "print(f\"差回复: {preference_data[0]['rejected'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 设置DPO训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型和tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 设置pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"模型加载完成: {model_name}\")\n",
    "print(f\"词汇表大小: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备数据集\n",
    "train_dataset = Dataset.from_list(preference_data)\n",
    "\n",
    "print(f\"训练数据集创建完成，大小: {len(train_dataset)}\")\n",
    "print(\"\\n数据集字段:\", train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数设置\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/dpo_gpt2\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,  # 小批次避免内存问题\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    logging_steps=5,\n",
    "    save_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    report_to=None,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_drop_last=True,\n",
    ")\n",
    "\n",
    "print(\"训练参数设置完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 开始DPO训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建DPO训练器\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,  # 将自动创建参考模型\n",
    "    args=training_args,\n",
    "    beta=0.1,  # DPO温度参数，控制与参考模型的偏差\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=128,\n",
    "    max_prompt_length=64,\n",
    ")\n",
    "\n",
    "print(\"✅ DPO训练器创建成功\")\n",
    "print(f\"Beta参数: {dpo_trainer.beta}\")\n",
    "print(f\"最大长度: {dpo_trainer.max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "print(\"🚀 开始DPO训练...\")\n",
    "\n",
    "# 训练模型\n",
    "dpo_trainer.train()\n",
    "\n",
    "print(\"✅ DPO训练完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 保存和测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "dpo_trainer.save_model()\n",
    "tokenizer.save_pretrained(\"./models/dpo_gpt2\")\n",
    "\n",
    "print(\"💾 模型已保存到 ./models/dpo_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试训练后的模型\n",
    "def generate_with_model(model, tokenizer, prompt, max_length=100):\n",
    "    \"\"\"使用模型生成文本\"\"\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 测试几个例子\n",
    "test_prompts = [\n",
    "    \"请解释什么是人工智能\",\n",
    "    \"给我一些学习编程的建议\", \n",
    "    \"如何提高写作能力\"\n",
    "]\n",
    "\n",
    "print(\"🧪 测试DPO训练后的模型:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = generate_with_model(model, tokenizer, prompt)\n",
    "    generated_part = response[len(prompt):].strip()\n",
    "    \n",
    "    print(f\"\\n📝 提示: {prompt}\")\n",
    "    print(f\"🤖 回复: {generated_part}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 深入理解DPO\n",
    "\n",
    "### DPO损失函数可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化DPO损失函数\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def dpo_loss(logits_diff, beta=0.1):\n",
    "    \"\"\"DPO损失函数\"\"\"\n",
    "    return -torch.log(torch.sigmoid(beta * logits_diff))\n",
    "\n",
    "# 创建数据点\n",
    "x = np.linspace(-5, 5, 100)\n",
    "betas = [0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for beta in betas:\n",
    "    y = [-np.log(1/(1 + np.exp(-beta * xi))) for xi in x]\n",
    "    plt.plot(x, y, label=f'β={beta}')\n",
    "\n",
    "plt.xlabel('log π(chosen) - log π(rejected)')\n",
    "plt.ylabel('DPO Loss')\n",
    "plt.title('DPO损失函数 vs β参数')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 DPO损失函数可视化完成\")\n",
    "print(\"💡 解读: β值越大，对偏好差异越敏感\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 模型对比实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 比较原始模型和DPO模型\n",
    "def compare_models():\n",
    "    \"\"\"比较原始模型和DPO训练后模型的输出质量\"\"\"\n",
    "    \n",
    "    # 加载原始模型\n",
    "    original_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "    original_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    if original_tokenizer.pad_token is None:\n",
    "        original_tokenizer.pad_token = original_tokenizer.eos_token\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"给我一个健康生活的建议\",\n",
    "        \"解释什么是深度学习\",\n",
    "        \"推荐一本好书\"\n",
    "    ]\n",
    "    \n",
    "    print(\"🔍 模型对比实验\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        print(f\"\\n🎯 测试 {i+1}: {prompt}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # 原始模型输出\n",
    "        original_output = generate_with_model(original_model, original_tokenizer, prompt)\n",
    "        original_response = original_output[len(prompt):].strip()\n",
    "        \n",
    "        # DPO模型输出\n",
    "        dpo_output = generate_with_model(model, tokenizer, prompt)\n",
    "        dpo_response = dpo_output[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"📖 原始GPT2: {original_response}\")\n",
    "        print(f\"🎯 DPO训练后: {dpo_response}\")\n",
    "\n",
    "# 运行对比\n",
    "compare_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. DPO vs PPO 性能分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析DPO的优势\n",
    "comparison_data = {\n",
    "    '训练阶段': ['PPO (RLHF)', 'DPO'],\n",
    "    '步骤数': [3, 2],\n",
    "    '需要奖励模型': ['是', '否'],\n",
    "    '训练稳定性': ['中等', '高'],\n",
    "    '实现复杂度': ['高', '低'],\n",
    "    '计算开销': ['高', '中等']\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "print(\"📊 DPO vs PPO 对比表:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# 绘制对比图\n",
    "metrics = ['简化程度', '稳定性', '效率', '易用性']\n",
    "ppo_scores = [6, 7, 6, 5]\n",
    "dpo_scores = [9, 9, 8, 9]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(x - width/2, ppo_scores, width, label='PPO (RLHF)', alpha=0.8)\n",
    "ax.bar(x + width/2, dpo_scores, width, label='DPO', alpha=0.8)\n",
    "\n",
    "ax.set_ylabel('评分')\n",
    "ax.set_title('DPO vs PPO 综合评估')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📈 DPO在多个维度上都优于传统RLHF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 交互式测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交互式测试DPO模型\n",
    "def interactive_test():\n",
    "    \"\"\"交互式测试DPO模型\"\"\"\n",
    "    \n",
    "    print(\"🎮 DPO模型交互测试\")\n",
    "    print(\"输入你的问题，模型会给出回答\")\n",
    "    print(\"(在notebook中运行此代码需要交互环境)\")\n",
    "    \n",
    "    # 这里提供几个测试样例\n",
    "    test_questions = [\n",
    "        \"如何学习机器学习\",\n",
    "        \"推荐一个编程项目\",\n",
    "        \"给我一些时间管理建议\"\n",
    "    ]\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"\\n❓ 问题: {question}\")\n",
    "        response = generate_with_model(model, tokenizer, question)\n",
    "        answer = response[len(question):].strip()\n",
    "        print(f\"🤖 DPO模型回答: {answer}\")\n",
    "\n",
    "# 运行交互测试\n",
    "interactive_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 总结和下一步\n",
    "\n",
    "通过这个教程，你已经学会了：\n",
    "\n",
    "✅ **DPO的核心概念**：直接偏好优化，无需奖励模型  \n",
    "✅ **数据准备**：如何构建偏好对比数据集  \n",
    "✅ **模型训练**：使用TRL进行DPO训练  \n",
    "✅ **效果评估**：比较训练前后的模型表现  \n",
    "\n",
    "### 下一步学习方向：\n",
    "1. 🔧 **高级定制**：自定义奖励函数和训练策略\n",
    "2. 🏭 **生产部署**：模型优化和部署最佳实践\n",
    "3. 📊 **评估指标**：更全面的模型评估方法\n",
    "4. 🎯 **实际应用**：将DPO应用到具体任务中"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}