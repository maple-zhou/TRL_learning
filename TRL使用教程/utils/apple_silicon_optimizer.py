#!/usr/bin/env python3
"""
è‹¹æœèŠ¯ç‰‡Macä¼˜åŒ–çš„è®¾å¤‡æ£€æµ‹å’Œé…ç½®å·¥å…·
"""

import torch
import platform
import psutil
from typing import Dict, Any

def detect_apple_silicon():
    """æ£€æµ‹æ˜¯å¦ä¸ºè‹¹æœèŠ¯ç‰‡"""
    try:
        # æ£€æŸ¥æ˜¯å¦ä¸ºmacOS ARM64
        return platform.system() == "Darwin" and platform.machine() == "arm64"
    except:
        return False

def get_optimal_device():
    """è·å–æœ€ä¼˜è®¡ç®—è®¾å¤‡"""
    if torch.backends.mps.is_available() and torch.backends.mps.is_built():
        return "mps"
    elif torch.cuda.is_available():
        return "cuda"
    else:
        return "cpu"

def get_device_info():
    """è·å–è¯¦ç»†è®¾å¤‡ä¿¡æ¯"""
    info = {
        "platform": platform.system(),
        "architecture": platform.machine(),
        "is_apple_silicon": detect_apple_silicon(),
        "python_version": platform.python_version(),
        "pytorch_version": torch.__version__,
    }
    
    # è®¾å¤‡æ”¯æŒ
    info["mps_available"] = torch.backends.mps.is_available()
    info["mps_built"] = torch.backends.mps.is_built() if hasattr(torch.backends.mps, 'is_built') else False
    info["cuda_available"] = torch.cuda.is_available()
    info["optimal_device"] = get_optimal_device()
    
    # å†…å­˜ä¿¡æ¯
    memory = psutil.virtual_memory()
    info["total_memory_gb"] = round(memory.total / (1024**3), 1)
    info["available_memory_gb"] = round(memory.available / (1024**3), 1)
    
    return info

def get_apple_silicon_config():
    """è·å–è‹¹æœèŠ¯ç‰‡ä¼˜åŒ–çš„è®­ç»ƒé…ç½®"""
    
    device_info = get_device_info()
    memory_gb = device_info["total_memory_gb"]
    
    # æ ¹æ®å†…å­˜å¤§å°è°ƒæ•´é…ç½®
    if memory_gb >= 32:
        # é«˜é…ç½® (32GB+)
        config = {
            "batch_size": 8,
            "mini_batch_size": 4,
            "gradient_accumulation_steps": 2,
            "max_length": 256,
        }
    elif memory_gb >= 16:
        # ä¸­ç­‰é…ç½® (16-32GB)
        config = {
            "batch_size": 4,
            "mini_batch_size": 2,
            "gradient_accumulation_steps": 4,
            "max_length": 128,
        }
    else:
        # ä½é…ç½® (8-16GB)
        config = {
            "batch_size": 2,
            "mini_batch_size": 1,
            "gradient_accumulation_steps": 8,
            "max_length": 64,
        }
    
    # é€šç”¨è‹¹æœèŠ¯ç‰‡ä¼˜åŒ–
    config.update({
        "learning_rate": 5e-6,          # ç¨å¾®é™ä½å­¦ä¹ ç‡
        "dataloader_num_workers": 0,    # è‹¹æœèŠ¯ç‰‡æ¨è
        "optimize_cuda_cache": False,   # å…³é—­CUDAä¼˜åŒ–
        "fp16": False,                  # MPSå¯èƒ½ä¸å®Œå…¨æ”¯æŒfp16
        "device": get_optimal_device(),
    })
    
    return config

def apply_apple_silicon_optimizations():
    """åº”ç”¨è‹¹æœèŠ¯ç‰‡ä¼˜åŒ–è®¾ç½®"""
    
    if detect_apple_silicon():
        print("ğŸ æ£€æµ‹åˆ°è‹¹æœèŠ¯ç‰‡ï¼Œåº”ç”¨ä¼˜åŒ–è®¾ç½®...")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        import os
        os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"  # å¯ç”¨MPSå›é€€
        os.environ["TOKENIZERS_PARALLELISM"] = "false"   # é¿å…tokenizerå¹¶è¡Œé—®é¢˜
        
        # æ˜¾ç¤ºå»ºè®®
        print("ğŸ’¡ è‹¹æœèŠ¯ç‰‡ä¼˜åŒ–å»ºè®®:")
        print("   - ä½¿ç”¨MPSè®¾å¤‡åŠ é€Ÿ")
        print("   - è®¾ç½®è¾ƒå°çš„batch_size")
        print("   - å…³é—­dataloaderå¤šè¿›ç¨‹")
        print("   - ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ")
        
        return True
    else:
        print("ğŸ–¥ï¸  éè‹¹æœèŠ¯ç‰‡è®¾å¤‡ï¼Œä½¿ç”¨æ ‡å‡†é…ç½®")
        return False

def show_device_recommendations():
    """æ˜¾ç¤ºè®¾å¤‡ç›¸å…³å»ºè®®"""
    
    info = get_device_info()
    config = get_apple_silicon_config()
    
    print("ğŸ” è®¾å¤‡ä¿¡æ¯å’Œå»ºè®®é…ç½®")
    print("=" * 50)
    
    print(f"å¹³å°: {info['platform']} {info['architecture']}")
    print(f"Python: {info['python_version']}")
    print(f"PyTorch: {info['pytorch_version']}")
    print(f"æ€»å†…å­˜: {info['total_memory_gb']}GB")
    print(f"å¯ç”¨å†…å­˜: {info['available_memory_gb']}GB")
    
    print(f"\nğŸ¯ æ¨èè®¾å¤‡: {info['optimal_device']}")
    
    if info["is_apple_silicon"]:
        print("ğŸ è‹¹æœèŠ¯ç‰‡ä¸“ç”¨ä¼˜åŒ–:")
        print(f"   - MPSå¯ç”¨: {info['mps_available']}")
        print(f"   - å»ºè®®batch_size: {config['batch_size']}")
        print(f"   - å»ºè®®max_length: {config['max_length']}")
    
    print(f"\nğŸ“Š æ¨èè®­ç»ƒé…ç½®:")
    for key, value in config.items():
        if key != "device":
            print(f"   {key}: {value}")

def create_device_specific_config():
    """åˆ›å»ºè®¾å¤‡ç‰¹å®šçš„é…ç½®æ–‡ä»¶"""
    
    config = get_apple_silicon_config()
    device_info = get_device_info()
    
    # ç”Ÿæˆé…ç½®å­—å…¸
    full_config = {
        "device_info": device_info,
        "training_config": config,
        "optimization_tips": [
            "ä½¿ç”¨è¾ƒå°çš„batch_sizeå‡å°‘å†…å­˜ä½¿ç”¨",
            "åˆ©ç”¨gradient_accumulation_stepsä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°", 
            "åœ¨è‹¹æœèŠ¯ç‰‡ä¸Šå…³é—­CUDAç›¸å…³ä¼˜åŒ–",
            "ç›‘æ§å†…å­˜ä½¿ç”¨ï¼Œé¿å…äº¤æ¢",
            "ä½¿ç”¨MPSè®¾å¤‡è·å¾—æœ€ä½³æ€§èƒ½"
        ]
    }
    
    # ä¿å­˜é…ç½®
    import json
    with open("apple_silicon_config.json", "w", encoding="utf-8") as f:
        json.dump(full_config, f, indent=2, ensure_ascii=False)
    
    print("ğŸ’¾ è®¾å¤‡é…ç½®å·²ä¿å­˜åˆ° apple_silicon_config.json")

def main():
    print("ğŸ è‹¹æœèŠ¯ç‰‡Macä¼˜åŒ–å·¥å…·")
    print("=" * 40)
    
    # æ£€æµ‹å¹¶ä¼˜åŒ–
    apply_apple_silicon_optimizations()
    
    # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
    show_device_recommendations()
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    create_device_specific_config()
    
    print("\nâœ… è‹¹æœèŠ¯ç‰‡ä¼˜åŒ–å®Œæˆï¼")
    print("ç°åœ¨å¯ä»¥æ­£å¸¸è¿è¡Œæ‰€æœ‰TRLæ•™ç¨‹äº†")

if __name__ == "__main__":
    main()