#!/usr/bin/env python3
"""
å®Œæ•´çš„TRLé¡¹ç›®å®æˆ˜ï¼šæ™ºèƒ½å®¢æœåŠ©æ‰‹
ä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹éƒ¨ç½²çš„å®Œæ•´æµç¨‹
"""

import torch
import json
import os
import numpy as np
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    Trainer, TrainingArguments,
    DataCollatorForLanguageModeling
)
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead, DPOTrainer
from datasets import Dataset
import gradio as gr
from typing import List, Dict, Optional
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CustomerServiceBot:
    """æ™ºèƒ½å®¢æœåŠ©æ‰‹ä¸»ç±»"""
    
    def __init__(self, base_model="gpt2"):
        self.base_model = base_model
        self.tokenizer = None
        self.model = None
        self.models_dir = "./project/models"
        os.makedirs(self.models_dir, exist_ok=True)
    
    def prepare_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # å®¢æœSFTæ•°æ®
        sft_data = [
            {
                "instruction": "ç”¨æˆ·è¯¢é—®é€€è´§æ”¿ç­–",
                "input": "è¯·é—®ä½ ä»¬çš„é€€è´§æ”¿ç­–æ˜¯ä»€ä¹ˆï¼Ÿ",
                "output": "æˆ‘ä»¬æ”¯æŒ7å¤©æ— ç†ç”±é€€è´§ã€‚å•†å“éœ€ä¿æŒåŸåŒ…è£…å®Œå¥½ï¼Œç”±æˆ‘ä»¬æ‰¿æ‹…è¿è´¹ã€‚æ‚¨å¯ä»¥é€šè¿‡å®¢æœæˆ–å®˜ç½‘ç”³è¯·é€€è´§ã€‚"
            },
            {
                "instruction": "ç”¨æˆ·å’¨è¯¢äº§å“ä¿¡æ¯",
                "input": "è¿™æ¬¾æ‰‹æœºçš„ç”µæ± ç»­èˆªæ€ä¹ˆæ ·ï¼Ÿ",
                "output": "è¿™æ¬¾æ‰‹æœºé…å¤‡4000mAhç”µæ± ï¼Œæ­£å¸¸ä½¿ç”¨å¯ç»­èˆªä¸€æ•´å¤©ã€‚æ”¯æŒå¿«å……åŠŸèƒ½ï¼Œ30åˆ†é’Ÿå¯å……ç”µ50%ã€‚"
            },
            {
                "instruction": "ç”¨æˆ·æŠ•è¯‰é—®é¢˜",
                "input": "æˆ‘æ”¶åˆ°çš„å•†å“æœ‰è´¨é‡é—®é¢˜",
                "output": "éå¸¸æŠ±æ­‰ç»™æ‚¨å¸¦æ¥ä¸ä¾¿ã€‚è¯·æä¾›è®¢å•å·å’Œé—®é¢˜ç…§ç‰‡ï¼Œæˆ‘ä»¬ä¼šç«‹å³ä¸ºæ‚¨å¤„ç†ï¼Œå…è´¹æ¢è´§æˆ–é€€æ¬¾ã€‚"
            },
            {
                "instruction": "ç”¨æˆ·è¯¢é—®é…é€",
                "input": "è®¢å•ä»€ä¹ˆæ—¶å€™èƒ½åˆ°ï¼Ÿ",
                "output": "è¯·æä¾›æ‚¨çš„è®¢å•å·ï¼Œæˆ‘å¸®æ‚¨æŸ¥è¯¢ç‰©æµä¿¡æ¯ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼ŒåŒåŸå½“æ—¥è¾¾ï¼Œå…¶ä»–åœ°åŒº1-3ä¸ªå·¥ä½œæ—¥é€è¾¾ã€‚"
            },
            {
                "instruction": "ç”¨æˆ·å’¨è¯¢ä¼˜æƒ ",
                "input": "æœ‰ä»€ä¹ˆä¼˜æƒ æ´»åŠ¨å—ï¼Ÿ",
                "output": "ç›®å‰æœ‰æ–°ç”¨æˆ·æ³¨å†Œç«‹å‡50å…ƒï¼Œæ»¡200å‡20çš„æ´»åŠ¨ã€‚å¦å¤–å…³æ³¨æˆ‘ä»¬å…¬ä¼—å·å¯è·å¾—ä¸“å±ä¼˜æƒ åˆ¸ã€‚"
            }
        ] * 50  # é‡å¤åˆ›å»ºæ›´å¤šæ ·æœ¬
        
        # åå¥½å¯¹æ¯”æ•°æ®
        preference_data = [
            {
                "prompt": "ç”¨æˆ·è¯¢é—®é€€è´§æ”¿ç­–",
                "chosen": "æˆ‘ä»¬æ”¯æŒ7å¤©æ— ç†ç”±é€€è´§ï¼Œå•†å“éœ€ä¿æŒåŸåŒ…è£…å®Œå¥½ã€‚é€€è´§è¿è´¹ç”±æˆ‘ä»¬æ‰¿æ‹…ï¼Œæ‚¨å¯ä»¥é€šè¿‡å®¢æœçƒ­çº¿æˆ–å®˜ç½‘ç”³è¯·ã€‚",
                "rejected": "é€€è´§é€€è´§é€€è´§ï¼Œ7å¤©7å¤©7å¤©ã€‚"
            },
            {
                "prompt": "ç”¨æˆ·å’¨è¯¢äº§å“ä¿¡æ¯", 
                "chosen": "è¿™æ¬¾äº§å“å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼šé«˜æ€§èƒ½å¤„ç†å™¨ã€é•¿ç»­èˆªç”µæ± ã€ä¼˜è´¨æ‘„åƒå¤´ã€‚é€‚åˆæ—¥å¸¸ä½¿ç”¨å’Œå•†åŠ¡åŠå…¬ã€‚",
                "rejected": "äº§å“å¾ˆå¥½å¾ˆå¥½å¾ˆå¥½å¾ˆå¥½ã€‚"
            },
            {
                "prompt": "ç”¨æˆ·æŠ•è¯‰é—®é¢˜",
                "chosen": "éå¸¸æŠ±æ­‰ç»™æ‚¨å¸¦æ¥å›°æ‰°ã€‚æˆ‘ä»¬ä¼šè®¤çœŸå¤„ç†æ‚¨çš„é—®é¢˜ï¼Œè¯·æä¾›è®¢å•ä¿¡æ¯ï¼Œæˆ‘ä»¬å°†å°½å¿«ä¸ºæ‚¨è§£å†³ã€‚",
                "rejected": "æŠ•è¯‰æŠ•è¯‰æŠ•è¯‰æŠ•è¯‰æŠ•è¯‰ã€‚"
            }
        ] * 30
        
        return sft_data, preference_data
    
    def run_supervised_fine_tuning(self, sft_data):
        """æ‰§è¡Œç›‘ç£å¾®è°ƒ"""
        logger.info("ğŸ¯ å¼€å§‹ç›‘ç£å¾®è°ƒ (SFT)...")
        
        # å‡†å¤‡tokenizerå’Œæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(self.base_model)
        
        # æ ¼å¼åŒ–æ•°æ®
        formatted_data = []
        for item in sft_data:
            text = f"å®¢æœå¯¹è¯\nç”¨æˆ·: {item['input']}\nå®¢æœ: {item['output']}{self.tokenizer.eos_token}"
            formatted_data.append({"text": text})
        
        dataset = Dataset.from_list(formatted_data)
        
        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"],
                truncation=True,
                padding="max_length",
                max_length=256
            )
        
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=f"{self.models_dir}/sft_model",
            overwrite_output_dir=True,
            num_train_epochs=3,
            per_device_train_batch_size=2,
            gradient_accumulation_steps=2,
            warmup_steps=20,
            logging_steps=10,
            save_steps=100,
            save_strategy="epoch",
            report_to=None,
        )
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        # è®­ç»ƒå™¨
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        trainer.save_model()
        self.tokenizer.save_pretrained(f"{self.models_dir}/sft_model")
        
        logger.info("âœ… SFTè®­ç»ƒå®Œæˆ")
        return f"{self.models_dir}/sft_model"
    
    def train_reward_model(self, preference_data):
        """è®­ç»ƒå®¢æœä¸“ç”¨å¥–åŠ±æ¨¡å‹"""
        logger.info("ğŸ¯ è®­ç»ƒå¥–åŠ±æ¨¡å‹...")
        
        class CustomerServiceRewardModel(nn.Module):
            def __init__(self, model_name):
                super().__init__()
                self.backbone = AutoModel.from_pretrained(model_name)
                hidden_size = self.backbone.config.hidden_size
                
                # å¤šå¤´å¥–åŠ±é¢„æµ‹
                self.helpfulness_head = nn.Linear(hidden_size, 1)
                self.politeness_head = nn.Linear(hidden_size, 1)
                self.accuracy_head = nn.Linear(hidden_size, 1)
                self.final_head = nn.Linear(3, 1)  # ç»¼åˆä¸‰ä¸ªç»´åº¦
                
            def forward(self, input_ids, attention_mask=None):
                outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
                pooled_output = outputs.last_hidden_state[:, -1, :]  # ä½¿ç”¨æœ€åä¸€ä¸ªtoken
                
                helpfulness = torch.sigmoid(self.helpfulness_head(pooled_output))
                politeness = torch.sigmoid(self.politeness_head(pooled_output))
                accuracy = torch.sigmoid(self.accuracy_head(pooled_output))
                
                # ç»¼åˆè¯„åˆ†
                combined_features = torch.cat([helpfulness, politeness, accuracy], dim=1)
                final_reward = self.final_head(combined_features)
                
                return final_reward, {
                    'helpfulness': helpfulness,
                    'politeness': politeness,
                    'accuracy': accuracy
                }
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        texts = []
        labels = []
        
        for item in preference_data:
            prompt = item["prompt"]
            chosen_text = f"å®¢æœå¯¹è¯\nç”¨æˆ·: {prompt}\nå®¢æœ: {item['chosen']}"
            rejected_text = f"å®¢æœå¯¹è¯\nç”¨æˆ·: {prompt}\nå®¢æœ: {item['rejected']}"
            
            texts.extend([chosen_text, rejected_text])
            labels.extend([1.0, 0.0])
        
        # è®­ç»ƒå¥–åŠ±æ¨¡å‹
        reward_model = CustomerServiceRewardModel(self.base_model)
        
        # è¿™é‡Œçœç•¥å…·ä½“è®­ç»ƒä»£ç ï¼Œå®é™…é¡¹ç›®ä¸­éœ€è¦å®Œæ•´å®ç°
        logger.info("âœ… å¥–åŠ±æ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        return reward_model
    
    def run_rlhf_training(self, preference_data):
        """è¿è¡ŒRLHFè®­ç»ƒ"""
        logger.info("ğŸ¯ å¼€å§‹RLHFè®­ç»ƒ...")
        
        # åŠ è½½SFTæ¨¡å‹
        try:
            model = AutoModelForCausalLMWithValueHead.from_pretrained(f"{self.models_dir}/sft_model")
            tokenizer = AutoTokenizer.from_pretrained(f"{self.models_dir}/sft_model")
        except:
            logger.warning("æœªæ‰¾åˆ°SFTæ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹")
            model = AutoModelForCausalLMWithValueHead.from_pretrained(self.base_model)
            tokenizer = AutoTokenizer.from_pretrained(self.base_model)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
        
        # PPOé…ç½®
        config = PPOConfig(
            model_name=self.base_model,
            learning_rate=1.41e-5,
            batch_size=8,
            mini_batch_size=2,
            steps=100,
            ppo_epochs=4,
            target_kl=0.1,
            cliprange=0.2,
            vf_coef=0.1,
        )
        
        # å‡†å¤‡æŸ¥è¯¢æ•°æ®
        queries = [item["prompt"] for item in preference_data[:50]]  # ä½¿ç”¨åå¥½æ•°æ®çš„æç¤º
        query_dataset = Dataset.from_dict({"query": queries})
        
        # å®¢æœä¸“ç”¨å¥–åŠ±å‡½æ•°
        def customer_service_reward(texts: List[str]) -> List[float]:
            """å®¢æœä¸“ç”¨å¥–åŠ±å‡½æ•°"""
            rewards = []
            
            service_keywords = {
                'polite': ['è¯·', 'æ‚¨', 'è°¢è°¢', 'æŠ±æ­‰', 'éº»çƒ¦', 'please', 'thank', 'sorry'],
                'helpful': ['å¸®åŠ©', 'è§£å†³', 'å¤„ç†', 'å»ºè®®', 'help', 'solve', 'suggest'],
                'professional': ['æ”¿ç­–', 'æµç¨‹', 'è§„å®š', 'policy', 'process', 'procedure']
            }
            
            for text in texts:
                reward = 0.1  # åŸºç¡€åˆ†
                text_lower = text.lower()
                
                # ç¤¼è²Œç”¨è¯­
                polite_count = sum(1 for word in service_keywords['polite'] if word in text_lower)
                reward += min(0.3, polite_count * 0.1)
                
                # æœ‰ç”¨æ€§
                helpful_count = sum(1 for word in service_keywords['helpful'] if word in text_lower)
                reward += min(0.4, helpful_count * 0.15)
                
                # ä¸“ä¸šæ€§
                prof_count = sum(1 for word in service_keywords['professional'] if word in text_lower)
                reward += min(0.3, prof_count * 0.1)
                
                # é•¿åº¦åˆç†æ€§
                if 30 <= len(text) <= 150:
                    reward += 0.2
                
                # é¿å…é‡å¤å†…å®¹
                words = text.split()
                if len(words) > 0:
                    unique_ratio = len(set(words)) / len(words)
                    if unique_ratio < 0.7:
                        reward -= 0.2
                
                rewards.append(max(0, min(1, reward)))
            
            return rewards
        
        # åˆ›å»ºPPOè®­ç»ƒå™¨
        ppo_trainer = PPOTrainer(
            config=config,
            model=model,
            ref_model=None,
            tokenizer=tokenizer,
            dataset=query_dataset,
        )
        
        logger.info("ğŸš€ å¼€å§‹PPOè®­ç»ƒ...")
        
        # è®­ç»ƒå¾ªç¯
        training_stats = []
        
        for epoch, batch in enumerate(ppo_trainer.dataloader):
            if epoch >= config.steps:
                break
            
            # ç”Ÿæˆå›å¤
            query_tensors = batch["input_ids"]
            response_tensors = ppo_trainer.generate(
                query_tensors,
                return_prompt=False,
                max_length=200,
                do_sample=True,
                top_p=0.9,
                temperature=0.7,
            )
            
            # è§£ç å¹¶è®¡ç®—å¥–åŠ±
            batch_texts = []
            for i in range(len(response_tensors)):
                response_text = tokenizer.decode(response_tensors[i], skip_special_tokens=True)
                batch_texts.append(response_text)
            
            rewards = customer_service_reward(batch_texts)
            rewards = [torch.tensor(r) for r in rewards]
            
            # PPOæ›´æ–°
            stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
            
            # è®°å½•ç»Ÿè®¡
            mean_reward = np.mean([r.item() for r in rewards])
            training_stats.append({
                'step': epoch,
                'mean_reward': mean_reward,
                'policy_loss': stats.get('ppo/loss/policy', 0) if stats else 0
            })
            
            if epoch % 10 == 0:
                logger.info(f"æ­¥éª¤ {epoch}: å¹³å‡å¥–åŠ± = {mean_reward:.3f}")
        
        # ä¿å­˜æ¨¡å‹
        model.save_pretrained(f"{self.models_dir}/rlhf_customer_service")
        tokenizer.save_pretrained(f"{self.models_dir}/rlhf_customer_service")
        
        logger.info("âœ… RLHFè®­ç»ƒå®Œæˆ")
        
        # ä¿å­˜è®­ç»ƒç»Ÿè®¡
        with open(f"{self.models_dir}/training_stats.json", "w") as f:
            json.dump(training_stats, f, indent=2)
        
        return training_stats
    
    def run_dpo_alternative(self, preference_data):
        """ä½¿ç”¨DPOä½œä¸ºRLHFçš„æ›¿ä»£æ–¹æ¡ˆ"""
        logger.info("ğŸ¯ å¼€å§‹DPOè®­ç»ƒï¼ˆRLHFæ›¿ä»£æ–¹æ¡ˆï¼‰...")
        
        # åŠ è½½SFTæ¨¡å‹
        try:
            model = AutoModelForCausalLM.from_pretrained(f"{self.models_dir}/sft_model")
            tokenizer = AutoTokenizer.from_pretrained(f"{self.models_dir}/sft_model")
        except:
            model = AutoModelForCausalLM.from_pretrained(self.base_model)
            tokenizer = AutoTokenizer.from_pretrained(self.base_model)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
        
        # æ ¼å¼åŒ–DPOæ•°æ®
        dpo_dataset = []
        for item in preference_data:
            dpo_item = {
                "prompt": f"å®¢æœå¯¹è¯\nç”¨æˆ·: {item['prompt']}\nå®¢æœ: ",
                "chosen": item["chosen"],
                "rejected": item["rejected"]
            }
            dpo_dataset.append(dpo_item)
        
        train_dataset = Dataset.from_list(dpo_dataset)
        
        # DPOè®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=f"{self.models_dir}/dpo_customer_service",
            num_train_epochs=2,
            per_device_train_batch_size=2,
            gradient_accumulation_steps=2,
            warmup_steps=10,
            logging_steps=5,
            save_strategy="epoch",
            report_to=None,
            remove_unused_columns=False,
        )
        
        # DPOè®­ç»ƒå™¨
        dpo_trainer = DPOTrainer(
            model=model,
            ref_model=None,
            args=training_args,
            beta=0.1,
            train_dataset=train_dataset,
            tokenizer=tokenizer,
            max_length=256,
            max_prompt_length=128,
        )
        
        # å¼€å§‹è®­ç»ƒ
        dpo_trainer.train()
        dpo_trainer.save_model()
        tokenizer.save_pretrained(f"{self.models_dir}/dpo_customer_service")
        
        logger.info("âœ… DPOè®­ç»ƒå®Œæˆ")
    
    def evaluate_model(self, model_path: str):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        logger.info(f"ğŸ“Š è¯„ä¼°æ¨¡å‹: {model_path}")
        
        # åŠ è½½æ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        if "rlhf" in model_path:
            model = AutoModelForCausalLMWithValueHead.from_pretrained(model_path)
        else:
            model = AutoModelForCausalLM.from_pretrained(model_path)
        
        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {
                "query": "æˆ‘æƒ³é€€è´§ï¼Œä½†æ˜¯è¶…è¿‡äº†7å¤©",
                "expected_type": "policy_explanation"
            },
            {
                "query": "ä½ ä»¬çš„äº§å“è´¨é‡æ€ä¹ˆæ ·ï¼Ÿ",
                "expected_type": "product_info"
            },
            {
                "query": "æˆ‘çš„è®¢å•å‡ºäº†é—®é¢˜",
                "expected_type": "problem_resolution"
            },
            {
                "query": "æœ‰ä¼˜æƒ æ´»åŠ¨å—ï¼Ÿ",
                "expected_type": "promotion_info"
            }
        ]
        
        results = []
        
        for test_case in test_cases:
            query = f"å®¢æœå¯¹è¯\nç”¨æˆ·: {test_case['query']}\nå®¢æœ: "
            inputs = tokenizer.encode(query, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_length=200,
                    do_sample=True,
                    top_p=0.9,
                    temperature=0.7,
                    pad_token_id=tokenizer.pad_token_id
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_part = response[len(query):].strip()
            
            # ç®€å•è´¨é‡è¯„ä¼°
            quality_score = self._evaluate_response_quality(generated_part)
            
            results.append({
                "query": test_case['query'],
                "response": generated_part,
                "quality_score": quality_score,
                "expected_type": test_case['expected_type']
            })
            
            print(f"\nâ“ æŸ¥è¯¢: {test_case['query']}")
            print(f"ğŸ¤– å›å¤: {generated_part}")
            print(f"ğŸ“Š è´¨é‡åˆ†: {quality_score:.2f}")
        
        return results
    
    def _evaluate_response_quality(self, response: str) -> float:
        """è¯„ä¼°å›å¤è´¨é‡"""
        score = 0.0
        
        # é•¿åº¦åˆç†æ€§
        if 20 <= len(response) <= 100:
            score += 0.3
        
        # åŒ…å«æœ‰ç”¨ä¿¡æ¯
        useful_patterns = ['å¯ä»¥', 'å»ºè®®', 'å¸®åŠ©', 'å¤„ç†', 'è”ç³»']
        for pattern in useful_patterns:
            if pattern in response:
                score += 0.1
        
        # ä¸“ä¸šæ€§
        if any(word in response for word in ['æ”¿ç­–', 'æµç¨‹', 'æœåŠ¡']):
            score += 0.2
        
        # ç¤¼è²Œæ€§
        if any(word in response for word in ['è¯·', 'è°¢è°¢', 'æŠ±æ­‰']):
            score += 0.2
        
        return min(1.0, score)
    
    def create_web_interface(self, model_path: str):
        """åˆ›å»ºWebç•Œé¢"""
        logger.info("ğŸŒ åˆ›å»ºWebç•Œé¢...")
        
        # åŠ è½½æœ€ç»ˆæ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        if "rlhf" in model_path:
            model = AutoModelForCausalLMWithValueHead.from_pretrained(model_path)
        else:
            model = AutoModelForCausalLM.from_pretrained(model_path)
        
        def chat_function(user_input: str, history: List[List[str]]) -> tuple:
            """èŠå¤©å‡½æ•°"""
            
            # æ„å»ºæç¤º
            prompt = f"å®¢æœå¯¹è¯\nç”¨æˆ·: {user_input}\nå®¢æœ: "
            inputs = tokenizer.encode(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_length=200,
                    do_sample=True,
                    top_p=0.9,
                    temperature=0.7,
                    pad_token_id=tokenizer.pad_token_id
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            bot_response = response[len(prompt):].strip()
            
            # æ›´æ–°å†å²
            history.append([user_input, bot_response])
            
            return "", history
        
        # åˆ›å»ºGradioç•Œé¢
        with gr.Blocks(title="æ™ºèƒ½å®¢æœåŠ©æ‰‹") as interface:
            gr.Markdown("# ğŸ¤– æ™ºèƒ½å®¢æœåŠ©æ‰‹")
            gr.Markdown("åŸºäºTRLè®­ç»ƒçš„å®¢æœæœºå™¨äººï¼Œå¯ä»¥å›ç­”äº§å“å’¨è¯¢ã€å¤„ç†å”®åé—®é¢˜ç­‰")
            
            chatbot = gr.Chatbot(label="å¯¹è¯", height=400)
            msg = gr.Textbox(label="è¾“å…¥æ‚¨çš„é—®é¢˜", placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...")
            clear = gr.Button("æ¸…é™¤å¯¹è¯")
            
            msg.submit(chat_function, [msg, chatbot], [msg, chatbot])
            clear.click(lambda: ([], ""), outputs=[chatbot, msg])
        
        return interface
    
    def run_complete_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹å®Œæ•´RLHFæµç¨‹...")
        
        # 1. å‡†å¤‡æ•°æ®
        sft_data, preference_data = self.prepare_data()
        
        # 2. ç›‘ç£å¾®è°ƒ
        sft_model_path = self.run_supervised_fine_tuning(sft_data)
        
        # 3. é€‰æ‹©è®­ç»ƒæ–¹æ³•
        choice = input("\né€‰æ‹©è®­ç»ƒæ–¹æ³•: (1)å®Œæ•´RLHF (2)DPOæ›¿ä»£æ–¹æ¡ˆ: ")
        
        if choice == "1":
            # 4a. è®­ç»ƒå¥–åŠ±æ¨¡å‹ + PPO
            reward_model = self.train_reward_model(preference_data)
            training_stats = self.run_rlhf_training(preference_data)
            final_model_path = f"{self.models_dir}/rlhf_customer_service"
        else:
            # 4b. DPOè®­ç»ƒ
            self.run_dpo_alternative(preference_data)
            final_model_path = f"{self.models_dir}/dpo_customer_service"
        
        # 5. è¯„ä¼°æ¨¡å‹
        print("\nğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ:")
        evaluation_results = self.evaluate_model(final_model_path)
        
        # 6. åˆ›å»ºæ¼”ç¤ºç•Œé¢
        create_demo = input("\næ˜¯å¦åˆ›å»ºWebæ¼”ç¤ºç•Œé¢? (y/n): ")
        if create_demo.lower() == 'y':
            interface = self.create_web_interface(final_model_path)
            interface.launch(share=False, server_name="127.0.0.1", server_port=7860)
        
        logger.info("ğŸ‰ å®Œæ•´é¡¹ç›®æµç¨‹å®Œæˆï¼")
        
        return {
            'sft_model': sft_model_path,
            'final_model': final_model_path,
            'evaluation': evaluation_results
        }

def main():
    print("ğŸ¯ TRLå®Œæ•´é¡¹ç›®å®æˆ˜ï¼šæ™ºèƒ½å®¢æœåŠ©æ‰‹")
    print("=" * 60)
    
    bot = CustomerServiceBot()
    
    choice = input("é€‰æ‹©æ‰§è¡Œæ¨¡å¼: (1)å®Œæ•´æµç¨‹ (2)ä»…è¯„ä¼°ç°æœ‰æ¨¡å‹ (3)ä»…åˆ›å»ºç•Œé¢: ")
    
    if choice == "1":
        results = bot.run_complete_pipeline()
        print(f"\nğŸ‰ é¡¹ç›®å®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {results['final_model']}")
        
    elif choice == "2":
        model_path = input("è¾“å…¥æ¨¡å‹è·¯å¾„: ")
        if os.path.exists(model_path):
            bot.evaluate_model(model_path)
        else:
            print("âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨")
            
    elif choice == "3":
        model_path = input("è¾“å…¥æ¨¡å‹è·¯å¾„: ")
        if os.path.exists(model_path):
            interface = bot.create_web_interface(model_path)
            print("ğŸŒ å¯åŠ¨Webç•Œé¢...")
            interface.launch()
        else:
            print("âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨")
    
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main()