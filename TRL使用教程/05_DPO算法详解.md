# DPO (Direct Preference Optimization) 算法详解

## 1. DPO算法原理

### 为什么需要DPO？
- RLHF训练复杂，需要奖励模型
- PPO训练不稳定，容易发散
- DPO直接优化偏好，无需奖励模型

### DPO核心思想
- 直接从偏好数据中学习
- 将偏好对比转化为分类问题
- 避免了奖励模型的中间步骤

### 数学原理

DPO的损失函数：
```
L_DPO = -E[(log σ(β log π_θ(y_w|x)/π_ref(y_w|x) - β log π_θ(y_l|x)/π_ref(y_l|x)))]
```

其中：
- y_w: 偏好的回复 (chosen)
- y_l: 不偏好的回复 (rejected)  
- π_θ: 当前策略
- π_ref: 参考策略
- β: 温度参数
- σ: sigmoid函数

## 2. DPO vs PPO 对比

| 特性 | PPO (RLHF) | DPO |
|------|------------|-----|
| 训练阶段 | 3阶段 (SFT→RM→PPO) | 2阶段 (SFT→DPO) |
| 奖励模型 | 需要 | 不需要 |
| 训练稳定性 | 中等 | 高 |
| 计算开销 | 高 | 中等 |
| 实现复杂度 | 高 | 低 |
| 效果 | 很好 | 很好 |

## 3. DPO的优势

### 简化训练流程
- 无需单独训练奖励模型
- 减少了一个训练阶段
- 降低了实现复杂度

### 提高训练稳定性
- 避免了PPO的不稳定性
- 直接优化目标函数
- 更容易调参和调试

### 保持性能
- 在多个基准测试中与RLHF性能相当
- 更快的收敛速度
- 更好的样本效率

## 4. 适用场景

### 适合DPO的情况
- 有足够的偏好对比数据
- 希望简化训练流程
- 计算资源有限
- 需要快速迭代

### 仍然选择PPO的情况
- 需要在线学习和适应
- 有复杂的奖励函数
- 需要与环境交互
- 对微调控制要求很高