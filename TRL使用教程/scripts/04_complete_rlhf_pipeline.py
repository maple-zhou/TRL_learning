#!/usr/bin/env python3
"""
å®Œæ•´çš„RLHFè®­ç»ƒæµç¨‹ç¤ºä¾‹
æ¼”ç¤ºä»SFTåˆ°å¥–åŠ±æ¨¡å‹è®­ç»ƒå†åˆ°PPOçš„å®Œæ•´æµç¨‹
"""

import torch
import json
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from datasets import Dataset
import os

class RLHFPipeline:
    """å®Œæ•´çš„RLHFè®­ç»ƒæµç¨‹"""
    
    def __init__(self, base_model="gpt2"):
        self.base_model = base_model
        self.tokenizer = AutoTokenizer.from_pretrained(base_model)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def create_sft_dataset(self):
        """åˆ›å»ºç›‘ç£å¾®è°ƒæ•°æ®é›†"""
        
        sft_data = [
            {
                "instruction": "è¯·å†™ä¸€é¦–å…³äºå‹è°Šçš„è¯—",
                "response": "å‹è°Šå¦‚æ˜¥é£ï¼Œæ¸©æš–äººå¿ƒç”°ã€‚\nçœŸè¯šç›¸å¾…ä¹…ï¼Œæ‚£éš¾è§çœŸæƒ…ã€‚"
            },
            {
                "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
                "response": "æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œè®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ å’Œæ”¹è¿›ï¼Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚"
            },
            {
                "instruction": "ç»™æˆ‘ä¸€äº›å­¦ä¹ ç¼–ç¨‹çš„å»ºè®®",
                "response": "å»ºè®®ä»åŸºç¡€è¯­æ³•å¼€å§‹ï¼Œå¤šåŠ¨æ‰‹å®è·µï¼Œå‚ä¸å¼€æºé¡¹ç›®ï¼ŒåšæŒæ¯å¤©ç¼–ç ç»ƒä¹ ã€‚"
            },
            {
                "instruction": "æè¿°ç†æƒ³çš„å·¥ä½œç¯å¢ƒ",
                "response": "ç†æƒ³çš„å·¥ä½œç¯å¢ƒåº”è¯¥æœ‰è‰¯å¥½çš„å›¢é˜Ÿæ°›å›´ã€å……è¶³çš„å­¦ä¹ æœºä¼šã€åˆç†çš„å·¥ä½œå¼ºåº¦å’Œæˆé•¿ç©ºé—´ã€‚"
            },
            {
                "instruction": "æ¨èä¸€äº›å¥åº·çš„ç”Ÿæ´»ä¹ æƒ¯",
                "response": "å»ºè®®ä¿æŒè§„å¾‹ä½œæ¯ã€å‡è¡¡é¥®é£Ÿã€é€‚é‡è¿åŠ¨ã€å……è¶³ç¡çœ å’Œç§¯æçš„å¿ƒæ€ã€‚"
            }
        ] * 20  # é‡å¤åˆ›å»ºæ›´å¤šæ ·æœ¬
        
        # æ ¼å¼åŒ–ä¸ºå¯¹è¯æ ¼å¼
        formatted_data = []
        for item in sft_data:
            text = f"æŒ‡ä»¤: {item['instruction']}\nå›å¤: {item['response']}{self.tokenizer.eos_token}"
            formatted_data.append({"text": text})
        
        return Dataset.from_list(formatted_data)
    
    def run_sft(self):
        """æ‰§è¡Œç›‘ç£å¾®è°ƒ"""
        
        print("ğŸ¯ é˜¶æ®µä¸€: ç›‘ç£å¾®è°ƒ (SFT)")
        
        # å‡†å¤‡æ•°æ®
        dataset = self.create_sft_dataset()
        
        def tokenize_function(examples):
            return self.tokenizer(
                examples["text"],
                truncation=True,
                padding="max_length",
                max_length=128
            )
        
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(self.base_model)
        model.resize_token_embeddings(len(self.tokenizer))
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir="./models/sft_model",
            overwrite_output_dir=True,
            num_train_epochs=2,
            per_device_train_batch_size=2,
            gradient_accumulation_steps=2,
            warmup_steps=10,
            logging_steps=5,
            save_steps=50,
            save_strategy="epoch",
            report_to=None,
            dataloader_drop_last=True,
        )
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )
        
        # å¼€å§‹è®­ç»ƒ
        print("ğŸš€ å¼€å§‹SFTè®­ç»ƒ...")
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        self.tokenizer.save_pretrained("./models/sft_model")
        
        print("âœ… SFTè®­ç»ƒå®Œæˆ")
        return "./models/sft_model"
    
    def create_preference_dataset(self):
        """åˆ›å»ºåå¥½æ•°æ®é›†ç”¨äºå¥–åŠ±æ¨¡å‹è®­ç»ƒ"""
        
        preference_data = [
            {
                "prompt": "è¯·å†™ä¸€é¦–å…³äºå‹è°Šçš„è¯—",
                "chosen": "å‹è°Šå¦‚æ˜¥é£ï¼Œæ¸©æš–äººå¿ƒç”°ã€‚\nçœŸè¯šç›¸å¾…ä¹…ï¼Œæ‚£éš¾è§çœŸæƒ…ã€‚",
                "rejected": "è¯—è¯—è¯—è¯—è¯—è¯—è¯—è¯—è¯—ã€‚"
            },
            {
                "prompt": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
                "chosen": "æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œè®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ ã€‚",
                "rejected": "æœºå™¨å­¦ä¹ å°±æ˜¯æœºå™¨å­¦ä¹ æœºå™¨å­¦ä¹ ã€‚"
            },
            {
                "prompt": "ç»™æˆ‘ä¸€äº›å­¦ä¹ å»ºè®®",
                "chosen": "å»ºè®®åˆ¶å®šå­¦ä¹ è®¡åˆ’ï¼Œä¿æŒæŒç»­ç»ƒä¹ ï¼Œå¤šæ€è€ƒæ€»ç»“ã€‚",
                "rejected": "å­¦å­¦å­¦å­¦å­¦å­¦å­¦å­¦å­¦ã€‚"
            },
            {
                "prompt": "æè¿°ç†æƒ³çš„å‡æœŸ",
                "chosen": "ç†æƒ³çš„å‡æœŸæ˜¯ä¸å®¶äººæœ‹å‹ä¸€èµ·ï¼Œæ”¾æ¾èº«å¿ƒï¼Œæ¢ç´¢æ–°åœ°æ–¹ã€‚",
                "rejected": "å‡æœŸå‡æœŸå‡æœŸå‡æœŸå‡æœŸã€‚"
            },
            {
                "prompt": "å¥åº·ç”Ÿæ´»çš„è¦ç´ æ˜¯ä»€ä¹ˆ",
                "chosen": "å¥åº·ç”Ÿæ´»éœ€è¦è§„å¾‹ä½œæ¯ã€å‡è¡¡é¥®é£Ÿã€é€‚é‡è¿åŠ¨å’Œç§¯æå¿ƒæ€ã€‚",
                "rejected": "å¥åº·å¥åº·å¥åº·å¥åº·å¥åº·ã€‚"
            }
        ] * 20
        
        return preference_data
    
    def train_reward_model(self):
        """è®­ç»ƒå¥–åŠ±æ¨¡å‹"""
        
        print("ğŸ¯ é˜¶æ®µäºŒ: å¥–åŠ±æ¨¡å‹è®­ç»ƒ")
        
        # åˆ›å»ºåå¥½æ•°æ®
        preference_data = self.create_preference_dataset()
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        texts = []
        labels = []
        
        for item in preference_data:
            prompt = item["prompt"]
            chosen_text = prompt + " " + item["chosen"]
            rejected_text = prompt + " " + item["rejected"]
            
            texts.extend([chosen_text, rejected_text])
            labels.extend([1.0, 0.0])  # chosen=1, rejected=0
        
        # Tokenize
        encodings = self.tokenizer(
            texts,
            truncation=True,
            padding="max_length",
            max_length=128,
            return_tensors="pt"
        )
        
        dataset = Dataset.from_dict({
            "input_ids": encodings["input_ids"],
            "attention_mask": encodings["attention_mask"],
            "labels": labels
        })
        
        # åˆ›å»ºå¥–åŠ±æ¨¡å‹
        reward_model = RewardModel(self.base_model)
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir="./models/reward_model",
            num_train_epochs=3,
            per_device_train_batch_size=4,
            logging_steps=10,
            save_strategy="epoch",
            report_to=None,
        )
        
        # è‡ªå®šä¹‰è®­ç»ƒå™¨
        class RewardTrainer(Trainer):
            def compute_loss(self, model, inputs, return_outputs=False):
                labels = inputs.pop("labels")
                outputs = model(**inputs)
                rewards = outputs.squeeze(-1)
                
                loss = nn.MSELoss()(
                    rewards, 
                    torch.tensor(labels, device=rewards.device, dtype=torch.float)
                )
                
                return (loss, outputs) if return_outputs else loss
        
        trainer = RewardTrainer(
            model=reward_model,
            args=training_args,
            train_dataset=dataset,
            tokenizer=self.tokenizer,
        )
        
        print("ğŸš€ å¼€å§‹å¥–åŠ±æ¨¡å‹è®­ç»ƒ...")
        trainer.train()
        trainer.save_model()
        
        print("âœ… å¥–åŠ±æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return reward_model
    
    def run_ppo_with_reward_model(self, reward_model):
        """ä½¿ç”¨å¥–åŠ±æ¨¡å‹è¿›è¡ŒPPOè®­ç»ƒ"""
        
        print("ğŸ¯ é˜¶æ®µä¸‰: PPOå¼ºåŒ–å­¦ä¹ ")
        
        # åŠ è½½SFTæ¨¡å‹
        try:
            model = AutoModelForCausalLMWithValueHead.from_pretrained("./models/sft_model")
        except:
            print("âš ï¸  æœªæ‰¾åˆ°SFTæ¨¡å‹ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹")
            model = AutoModelForCausalLMWithValueHead.from_pretrained(self.base_model)
        
        # PPOé…ç½®
        config = PPOConfig(
            model_name=self.base_model,
            learning_rate=1.41e-5,
            batch_size=4,
            mini_batch_size=2,
            steps=20,
            ppo_epochs=4,
            target_kl=0.1,
        )
        
        # åˆ›å»ºæŸ¥è¯¢æ•°æ®é›†
        queries = [
            "è¯·å†™ä¸€é¦–å…³äºå‹è°Šçš„è¯—ï¼š",
            "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼š",
            "ç»™æˆ‘ä¸€äº›å­¦ä¹ å»ºè®®ï¼š",
            "æè¿°ç†æƒ³çš„å‡æœŸï¼š",
            "å¥åº·ç”Ÿæ´»çš„è¦ç´ æ˜¯ä»€ä¹ˆï¼š",
        ] * 10
        
        query_dataset = Dataset.from_dict({"query": queries})
        
        # åˆ›å»ºPPOè®­ç»ƒå™¨
        ppo_trainer = PPOTrainer(
            config=config,
            model=model,
            ref_model=None,
            tokenizer=self.tokenizer,
            dataset=query_dataset,
        )
        
        def compute_rewards_with_model(texts):
            """ä½¿ç”¨å¥–åŠ±æ¨¡å‹è®¡ç®—å¥–åŠ±"""
            rewards = []
            
            for text in texts:
                encoding = self.tokenizer(
                    text,
                    return_tensors="pt",
                    truncation=True,
                    padding=True,
                    max_length=128
                )
                
                with torch.no_grad():
                    reward = reward_model(**encoding)
                    rewards.append(reward.squeeze().item())
            
            return rewards
        
        print("ğŸš€ å¼€å§‹PPOè®­ç»ƒ...")
        
        # PPOè®­ç»ƒå¾ªç¯
        for epoch, batch in enumerate(ppo_trainer.dataloader):
            if epoch >= config.steps:
                break
            
            print(f"\n--- PPOæ­¥éª¤ {epoch + 1} ---")
            
            # ç”Ÿæˆå›å¤
            query_tensors = batch["input_ids"]
            response_tensors = ppo_trainer.generate(
                query_tensors,
                return_prompt=False,
                max_length=100,
                do_sample=True,
                top_p=0.9,
                temperature=0.7,
            )
            
            # è§£ç æ–‡æœ¬
            batch_texts = []
            for i in range(len(response_tensors)):
                query_text = self.tokenizer.decode(query_tensors[i], skip_special_tokens=True)
                response_text = self.tokenizer.decode(response_tensors[i], skip_special_tokens=True)
                full_text = query_text + " " + response_text
                batch_texts.append(full_text)
                
                if i == 0:  # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬
                    print(f"æŸ¥è¯¢: {query_text}")
                    print(f"å›å¤: {response_text}")
            
            # ä½¿ç”¨å¥–åŠ±æ¨¡å‹è®¡ç®—å¥–åŠ±
            rewards = compute_rewards_with_model(batch_texts)
            rewards = [torch.tensor(r) for r in rewards]
            
            print(f"å¹³å‡å¥–åŠ±: {np.mean([r.item() for r in rewards]):.3f}")
            
            # PPOæ›´æ–°
            stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
            
            if stats:
                print(f"ç­–ç•¥æŸå¤±: {stats.get('ppo/loss/policy', 'N/A')}")
        
        print("âœ… PPOè®­ç»ƒå®Œæˆ")
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        model.save_pretrained("./models/rlhf_final_model")
        print("ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜")

def main():
    pipeline = RLHFPipeline()
    
    print("ğŸ¯ å®Œæ•´RLHFè®­ç»ƒæµç¨‹")
    print("è¿™ä¸ªæµç¨‹åŒ…å«ä¸‰ä¸ªé˜¶æ®µ:")
    print("1. ç›‘ç£å¾®è°ƒ (SFT)")
    print("2. å¥–åŠ±æ¨¡å‹è®­ç»ƒ (RM)")
    print("3. PPOå¼ºåŒ–å­¦ä¹  (RL)")
    
    choice = input("\né€‰æ‹©æ‰§è¡Œæ¨¡å¼: (1)å®Œæ•´æµç¨‹ (2)å•ç‹¬é˜¶æ®µ: ")
    
    if choice == "1":
        print("\nğŸš€ æ‰§è¡Œå®Œæ•´RLHFæµç¨‹...")
        
        # é˜¶æ®µ1: SFT
        sft_model_path = pipeline.run_sft()
        
        # é˜¶æ®µ2: å¥–åŠ±æ¨¡å‹
        reward_model = pipeline.train_reward_model()
        
        # é˜¶æ®µ3: PPO
        pipeline.run_ppo_with_reward_model(reward_model)
        
        print("\nğŸ‰ å®Œæ•´RLHFæµç¨‹æ‰§è¡Œå®Œæˆï¼")
        
    elif choice == "2":
        stage = input("é€‰æ‹©é˜¶æ®µ: (1)SFT (2)å¥–åŠ±æ¨¡å‹ (3)PPO: ")
        
        if stage == "1":
            pipeline.run_sft()
        elif stage == "2":
            reward_model = pipeline.train_reward_model()
        elif stage == "3":
            # éœ€è¦å…ˆåŠ è½½å¥–åŠ±æ¨¡å‹
            try:
                from scripts.reward_model_training import RewardModel
                reward_model = RewardModel()
                reward_model.load_state_dict(torch.load("./models/reward_model/pytorch_model.bin"))
                pipeline.run_ppo_with_reward_model(reward_model)
            except Exception as e:
                print(f"âŒ åŠ è½½å¥–åŠ±æ¨¡å‹å¤±è´¥: {e}")
                print("è¯·å…ˆè®­ç»ƒå¥–åŠ±æ¨¡å‹")
    else:
        print("æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main()