#!/usr/bin/env python3
"""
DPOè®­ç»ƒç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨TRLè¿›è¡ŒDPO (Direct Preference Optimization) è®­ç»ƒ
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from trl import DPOTrainer, DPOConfig
from datasets import Dataset
import json

def create_dpo_dataset():
    """åˆ›å»ºDPOè®­ç»ƒæ•°æ®é›†"""
    
    # DPOéœ€è¦åå¥½å¯¹æ¯”æ•°æ®ï¼šchosen vs rejected
    dpo_data = [
        {
            "prompt": "è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ",
            "chosen": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ï¼Œèƒ½å¤Ÿè‡ªåŠ¨ä»æ•°æ®ä¸­æå–ç‰¹å¾å’Œæ¨¡å¼ã€‚",
            "rejected": "æ·±åº¦å­¦ä¹ å°±æ˜¯å¾ˆæ·±çš„å­¦ä¹ ï¼Œå­¦å¾—å¾ˆæ·±å¾ˆæ·±çš„é‚£ç§å­¦ä¹ ã€‚"
        },
        {
            "prompt": "æ¨èä¸€æœ¬ç¼–ç¨‹ä¹¦ç±",
            "chosen": "æˆ‘æ¨èã€ŠPythonç¼–ç¨‹ï¼šä»å…¥é—¨åˆ°å®è·µã€‹ï¼Œè¿™æœ¬ä¹¦ç»“æ„æ¸…æ™°ï¼Œä¾‹å­ä¸°å¯Œï¼Œé€‚åˆåˆå­¦è€…ç³»ç»Ÿå­¦ä¹ Pythonã€‚",
            "rejected": "ä¹¦ä¹¦ä¹¦ï¼Œæ¨èä¹¦ä¹¦ä¹¦ä¹¦ä¹¦ä¹¦ä¹¦ã€‚"
        },
        {
            "prompt": "å¦‚ä½•ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼",
            "chosen": "ä¿æŒå¥åº·éœ€è¦ï¼š1)è§„å¾‹ä½œæ¯ï¼Œæ—©ç¡æ—©èµ·ï¼›2)å‡è¡¡é¥®é£Ÿï¼Œå¤šåƒè”¬æœï¼›3)é€‚é‡è¿åŠ¨ï¼Œæ¯å‘¨è‡³å°‘150åˆ†é’Ÿï¼›4)ç®¡ç†å‹åŠ›ï¼Œä¿æŒè‰¯å¥½å¿ƒæ€ã€‚",
            "rejected": "å¥åº·å°±æ˜¯å¥åº·å¥åº·å¥åº·å¥åº·å¥åº·ã€‚"
        },
        {
            "prompt": "æè¿°ä½ ç†æƒ³ä¸­çš„å·¥ä½œ",
            "chosen": "ç†æƒ³çš„å·¥ä½œåº”è¯¥æœ‰æŒ‘æˆ˜æ€§å’Œæˆé•¿æœºä¼šï¼Œå›¢é˜Ÿæ°›å›´èæ´½ï¼Œèƒ½å¤Ÿå¹³è¡¡å·¥ä½œä¸ç”Ÿæ´»ï¼ŒåŒæ—¶å¯¹ç¤¾ä¼šæœ‰ç§¯æè´¡çŒ®ã€‚",
            "rejected": "å·¥ä½œå·¥ä½œå·¥ä½œå·¥ä½œå·¥ä½œå·¥ä½œå·¥ä½œã€‚"
        },
        {
            "prompt": "ç»™åˆå­¦è€…çš„å­¦ä¹ å»ºè®®",
            "chosen": "å»ºè®®åˆå­¦è€…ï¼šåˆ¶å®šæ˜ç¡®ç›®æ ‡ï¼Œä¿æŒè€å¿ƒå’ŒåšæŒï¼Œå¤šå®è·µå°‘ç†è®ºï¼Œé‡åˆ°å›°éš¾è¦ä¸»åŠ¨å¯»æ±‚å¸®åŠ©ï¼Œå»ºç«‹è‰¯å¥½çš„å­¦ä¹ ä¹ æƒ¯ã€‚",
            "rejected": "å­¦å­¦å­¦å­¦å­¦å­¦å­¦å­¦å­¦å­¦å­¦å­¦ã€‚"
        }
    ] * 20  # é‡å¤åˆ›å»ºæ›´å¤šæ ·æœ¬
    
    return dpo_data

def format_dpo_data(data, tokenizer):
    """æ ¼å¼åŒ–DPOæ•°æ®"""
    
    formatted_data = []
    
    for item in data:
        # DPOè®­ç»ƒå™¨æœŸæœ›çš„æ•°æ®æ ¼å¼
        formatted_item = {
            "prompt": item["prompt"],
            "chosen": item["chosen"],
            "rejected": item["rejected"]
        }
        formatted_data.append(formatted_item)
    
    return Dataset.from_list(formatted_data)

def train_dpo_model():
    """è®­ç»ƒDPOæ¨¡å‹"""
    
    print("ğŸ¯ å¼€å§‹DPOè®­ç»ƒ")
    
    # 1. å‡†å¤‡æ¨¡å‹å’Œtokenizer
    model_name = "gpt2"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = tokenizer.pad_token_id
    
    # 2. å‡†å¤‡æ•°æ®
    raw_data = create_dpo_dataset()
    train_dataset = format_dpo_data(raw_data, tokenizer)
    
    print(f"è®­ç»ƒæ•°æ®å¤§å°: {len(train_dataset)}")
    print(f"ç¤ºä¾‹æ•°æ®: {train_dataset[0]}")
    
    # 3. DPOé…ç½®
    training_args = TrainingArguments(
        output_dir="./models/dpo_model",
        num_train_epochs=1,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=2,
        warmup_steps=10,
        logging_steps=5,
        save_steps=50,
        save_strategy="epoch",
        evaluation_strategy="no",
        report_to=None,
        remove_unused_columns=False,
        dataloader_drop_last=True,
    )
    
    # 4. åˆ›å»ºDPOè®­ç»ƒå™¨
    dpo_trainer = DPOTrainer(
        model=model,
        ref_model=None,  # å°†ä½¿ç”¨æ¨¡å‹å‰¯æœ¬ä½œä¸ºå‚è€ƒ
        args=training_args,
        beta=0.1,  # DPOæ¸©åº¦å‚æ•°
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        max_length=128,
        max_prompt_length=64,
    )
    
    print("ğŸš€ å¼€å§‹DPOè®­ç»ƒ...")
    
    # 5. å¼€å§‹è®­ç»ƒ
    dpo_trainer.train()
    
    # 6. ä¿å­˜æ¨¡å‹
    dpo_trainer.save_model()
    tokenizer.save_pretrained("./models/dpo_model")
    
    print("âœ… DPOè®­ç»ƒå®Œæˆ")
    print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ° ./models/dpo_model")

def test_dpo_model():
    """æµ‹è¯•DPOè®­ç»ƒåçš„æ¨¡å‹"""
    
    try:
        print("ğŸ§ª æµ‹è¯•DPOæ¨¡å‹...")
        
        # åŠ è½½æ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained("./models/dpo_model")
        model = AutoModelForCausalLM.from_pretrained("./models/dpo_model")
        
        test_prompts = [
            "è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ",
            "æ¨èä¸€æœ¬ç¼–ç¨‹ä¹¦ç±", 
            "å¦‚ä½•ä¿æŒå¥åº·çš„ç”Ÿæ´»æ–¹å¼",
            "ç»™åˆå­¦è€…çš„å­¦ä¹ å»ºè®®"
        ]
        
        print("\nğŸ¯ DPOæ¨¡å‹ç”Ÿæˆç»“æœ:")
        print("=" * 50)
        
        for prompt in test_prompts:
            inputs = tokenizer.encode(prompt, return_tensors="pt")
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_length=100,
                    do_sample=True,
                    top_p=0.9,
                    temperature=0.7,
                    pad_token_id=tokenizer.pad_token_id
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_part = response[len(prompt):].strip()
            
            print(f"\næç¤º: {prompt}")
            print(f"å›å¤: {generated_part}")
            print("-" * 30)
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("è¯·å…ˆè¿è¡ŒDPOè®­ç»ƒ")

def compare_with_original():
    """æ¯”è¾ƒDPOè®­ç»ƒå‰åçš„å·®å¼‚"""
    
    print("ğŸ” æ¯”è¾ƒåŸå§‹æ¨¡å‹å’ŒDPOæ¨¡å‹")
    
    # åŠ è½½åŸå§‹æ¨¡å‹
    original_tokenizer = AutoTokenizer.from_pretrained("gpt2")
    original_model = AutoModelForCausalLM.from_pretrained("gpt2")
    
    if original_tokenizer.pad_token is None:
        original_tokenizer.pad_token = original_tokenizer.eos_token
    
    try:
        # åŠ è½½DPOæ¨¡å‹
        dpo_tokenizer = AutoTokenizer.from_pretrained("./models/dpo_model")
        dpo_model = AutoModelForCausalLM.from_pretrained("./models/dpo_model")
        
        test_prompt = "ç»™åˆå­¦è€…çš„å­¦ä¹ å»ºè®®"
        
        print(f"\næµ‹è¯•æç¤º: '{test_prompt}'")
        print("=" * 60)
        
        # åŸå§‹æ¨¡å‹ç”Ÿæˆ
        inputs = original_tokenizer.encode(test_prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = original_model.generate(
                inputs, max_length=80, do_sample=True, 
                top_p=0.9, temperature=0.7,
                pad_token_id=original_tokenizer.pad_token_id
            )
        original_response = original_tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # DPOæ¨¡å‹ç”Ÿæˆ
        inputs = dpo_tokenizer.encode(test_prompt, return_tensors="pt")
        with torch.no_grad():
            outputs = dpo_model.generate(
                inputs, max_length=80, do_sample=True,
                top_p=0.9, temperature=0.7,
                pad_token_id=dpo_tokenizer.pad_token_id
            )
        dpo_response = dpo_tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print("ğŸ“– åŸå§‹GPT2:")
        print(original_response[len(test_prompt):].strip())
        
        print("\nğŸ¯ DPOè®­ç»ƒå:")
        print(dpo_response[len(test_prompt):].strip())
        
    except Exception as e:
        print(f"âŒ æ¯”è¾ƒå¤±è´¥: {e}")
        print("è¯·å…ˆè®­ç»ƒDPOæ¨¡å‹")

def main():
    print("ğŸ¯ DPO (Direct Preference Optimization) ç¤ºä¾‹")
    
    while True:
        print("\né€‰æ‹©æ“ä½œ:")
        print("1. è®­ç»ƒDPOæ¨¡å‹")
        print("2. æµ‹è¯•DPOæ¨¡å‹")
        print("3. æ¯”è¾ƒè®­ç»ƒå‰åæ•ˆæœ")
        print("4. é€€å‡º")
        
        choice = input("è¯·é€‰æ‹© (1-4): ")
        
        if choice == "1":
            train_dpo_model()
        elif choice == "2":
            test_dpo_model()
        elif choice == "3":
            compare_with_original()
        elif choice == "4":
            print("ğŸ‘‹ é€€å‡º")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main()