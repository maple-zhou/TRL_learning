#!/usr/bin/env python3
"""
å¥–åŠ±æ¨¡å‹è®­ç»ƒç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•è®­ç»ƒä¸€ä¸ªç®€å•çš„å¥–åŠ±æ¨¡å‹æ¥è¯„ä¼°æ–‡æœ¬è´¨é‡
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments
from datasets import Dataset
import numpy as np
from sklearn.metrics import accuracy_score

class RewardModel(nn.Module):
    """ç®€å•çš„å¥–åŠ±æ¨¡å‹"""
    
    def __init__(self, model_name="gpt2"):
        super().__init__()
        self.backbone = AutoModel.from_pretrained(model_name)
        self.reward_head = nn.Linear(self.backbone.config.hidden_size, 1)
        
    def forward(self, input_ids, attention_mask=None):
        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
        # ä½¿ç”¨æœ€åä¸€ä¸ªtokençš„hidden state
        last_hidden_state = outputs.last_hidden_state[:, -1, :]
        reward = self.reward_head(last_hidden_state)
        return reward

def create_preference_dataset():
    """åˆ›å»ºåå¥½å¯¹æ¯”æ•°æ®é›†"""
    
    # ç¤ºä¾‹æ•°æ®ï¼šæ¯ä¸ªæ ·æœ¬åŒ…å«chosen(å¥½çš„å›å¤)å’Œrejected(å·®çš„å›å¤)
    data = [
        {
            "prompt": "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œ",
            "chosen": "æˆ‘æƒ³å‡ºå»æ•£æ­¥ï¼Œäº«å—è¿™ç¾å¥½çš„é˜³å…‰ã€‚",
            "rejected": "å‘ƒå‘ƒå‘ƒå‘ƒå‘ƒå‘ƒå‘ƒå‘ƒå‘ƒå‘ƒå‘ƒã€‚"
        },
        {
            "prompt": "æ¨èä¸€æœ¬å¥½ä¹¦ï¼š",
            "chosen": "æˆ‘æ¨èã€Šä¸‰ä½“ã€‹ï¼Œè¿™æ˜¯ä¸€éƒ¨ä¼˜ç§€çš„ç§‘å¹»å°è¯´ï¼Œæƒ…èŠ‚å¼•äººå…¥èƒœã€‚",
            "rejected": "ä¹¦ä¹¦ä¹¦ä¹¦ä¹¦ä¹¦ä¹¦ä¹¦ä¹¦ä¹¦ã€‚"
        },
        {
            "prompt": "è§£é‡Šä»€ä¹ˆæ˜¯AIï¼š",
            "chosen": "AIæ˜¯äººå·¥æ™ºèƒ½çš„ç¼©å†™ï¼Œæ˜¯æ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„è®¡ç®—æœºç³»ç»Ÿã€‚",
            "rejected": "AIå°±æ˜¯AIå•ŠAIå°±æ˜¯AIã€‚"
        },
        {
            "prompt": "ç»™ä¸ªå¥åº·å»ºè®®ï¼š",
            "chosen": "å»ºè®®æ¯å¤©ä¿æŒé€‚é‡è¿åŠ¨ï¼Œå‡è¡¡é¥®é£Ÿï¼Œå……è¶³ç¡çœ ã€‚",
            "rejected": "åƒåƒåƒç¡ç¡ç¡ç©ç©ç©ã€‚"
        },
        {
            "prompt": "æè¿°æ˜¥å¤©ï¼š",
            "chosen": "æ˜¥å¤©ä¸‡ç‰©å¤è‹ï¼ŒèŠ±å„¿ç»½æ”¾ï¼Œæ¸©æš–çš„é˜³å…‰æ´’åœ¨å¤§åœ°ä¸Šã€‚",
            "rejected": "æ˜¥å¤©å°±æ˜¯æ˜¥å¤©æ˜¥å¤©æ˜¥å¤©æ˜¥å¤©ã€‚"
        }
    ] * 20  # é‡å¤åˆ›å»ºæ›´å¤šæ ·æœ¬
    
    return data

def prepare_training_data(preference_data, tokenizer):
    """å‡†å¤‡è®­ç»ƒæ•°æ®"""
    
    all_texts = []
    all_labels = []
    
    for item in preference_data:
        prompt = item["prompt"]
        chosen = prompt + item["chosen"]
        rejected = prompt + item["rejected"]
        
        all_texts.extend([chosen, rejected])
        all_labels.extend([1.0, 0.0])  # chosen=1, rejected=0
    
    # Tokenize
    encodings = tokenizer(
        all_texts,
        truncation=True,
        padding=True,
        max_length=128,
        return_tensors="pt"
    )
    
    return Dataset.from_dict({
        "input_ids": encodings["input_ids"],
        "attention_mask": encodings["attention_mask"],
        "labels": all_labels
    })

class RewardTrainer(Trainer):
    """è‡ªå®šä¹‰å¥–åŠ±æ¨¡å‹è®­ç»ƒå™¨"""
    
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        rewards = outputs.squeeze(-1)
        
        # ä½¿ç”¨MSEæŸå¤±
        loss = nn.MSELoss()(rewards, torch.tensor(labels, device=rewards.device, dtype=torch.float))
        
        return (loss, outputs) if return_outputs else loss

def train_reward_model():
    """è®­ç»ƒå¥–åŠ±æ¨¡å‹"""
    
    print("ğŸ¯ å¼€å§‹è®­ç»ƒå¥–åŠ±æ¨¡å‹...")
    
    # 1. å‡†å¤‡æ•°æ®
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    preference_data = create_preference_dataset()
    train_dataset = prepare_training_data(preference_data, tokenizer)
    
    print(f"è®­ç»ƒæ•°æ®å¤§å°: {len(train_dataset)}")
    
    # 2. åˆ›å»ºæ¨¡å‹
    model = RewardModel("gpt2")
    
    # 3. è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir="./models/reward_model",
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=1,
        warmup_steps=10,
        logging_steps=10,
        save_steps=100,
        evaluation_strategy="no",
        save_strategy="epoch",
        load_best_model_at_end=False,
        report_to=None,  # ä¸ä½¿ç”¨wandb
    )
    
    # 4. åˆ›å»ºè®­ç»ƒå™¨
    trainer = RewardTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
    )
    
    # 5. å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # 6. ä¿å­˜æ¨¡å‹
    trainer.save_model()
    print("ğŸ’¾ å¥–åŠ±æ¨¡å‹å·²ä¿å­˜")
    
    return model, tokenizer

def test_reward_model():
    """æµ‹è¯•è®­ç»ƒå¥½çš„å¥–åŠ±æ¨¡å‹"""
    
    try:
        print("ğŸ§ª æµ‹è¯•å¥–åŠ±æ¨¡å‹...")
        
        # åŠ è½½æ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained("./models/reward_model")
        model = RewardModel()
        model.load_state_dict(torch.load("./models/reward_model/pytorch_model.bin"))
        model.eval()
        
        # æµ‹è¯•æ ·æœ¬
        test_texts = [
            "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘æƒ³å‡ºå»æ•£æ­¥ã€‚",  # å¥½çš„å›å¤
            "å‘ƒå‘ƒå‘ƒå‘ƒå‘ƒå‘ƒå‘ƒå‘ƒå‘ƒã€‚",         # å·®çš„å›å¤
            "è¿™æ˜¯ä¸€ä¸ªæœ‰æ„ä¹‰çš„å›ç­”ã€‚",       # ä¸­ç­‰å›å¤
        ]
        
        print("\nå¥–åŠ±æ¨¡å‹è¯„åˆ†ç»“æœ:")
        for text in test_texts:
            encoding = tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=128
            )
            
            with torch.no_grad():
                reward = model(**encoding)
                print(f"'{text}' -> å¥–åŠ±: {reward.item():.3f}")
                
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒ")

def main():
    print("ğŸ¯ å¥–åŠ±æ¨¡å‹è®­ç»ƒç¤ºä¾‹")
    
    choice = input("é€‰æ‹©æ“ä½œ: (1)è®­ç»ƒå¥–åŠ±æ¨¡å‹ (2)æµ‹è¯•ç°æœ‰æ¨¡å‹: ")
    
    if choice == "1":
        train_reward_model()
        print("\nè®­ç»ƒå®Œæˆï¼Œå¯ä»¥è¿è¡Œæµ‹è¯•äº†")
    elif choice == "2":
        test_reward_model()
    else:
        print("æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main()