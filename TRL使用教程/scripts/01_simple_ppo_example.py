#!/usr/bin/env python3
"""
ç®€å•çš„PPOè®­ç»ƒç¤ºä¾‹
è¿™ä¸ªè„šæœ¬æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨TRLè¿›è¡ŒåŸºç¡€çš„PPOè®­ç»ƒ
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from datasets import Dataset
import numpy as np

def create_sample_dataset():
    """åˆ›å»ºä¸€ä¸ªç®€å•çš„ç¤ºä¾‹æ•°æ®é›†"""
    prompts = [
        "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ï¼š",
        "è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼š",
        "æ¨èä¸€æœ¬å¥½ä¹¦ï¼š",
        "æè¿°ä½ çš„ç†æƒ³å‡æœŸï¼š",
        "ç»™æˆ‘ä¸€ä¸ªå¥åº·é¥®é£Ÿå»ºè®®ï¼š",
    ] * 10  # é‡å¤åˆ›å»º50ä¸ªæ ·æœ¬
    
    return Dataset.from_dict({"query": prompts})

def simple_reward_function(texts):
    """
    ç®€å•çš„å¥–åŠ±å‡½æ•°ç¤ºä¾‹
    æ ¹æ®æ–‡æœ¬é•¿åº¦å’Œç§¯æè¯æ±‡ç»™å‡ºå¥–åŠ±
    """
    rewards = []
    positive_words = ["å¥½", "æ£’", "ä¼˜ç§€", "ç¾ä¸½", "å¿«ä¹", "å¥åº·", "æ¨è", "excellent", "good", "great"]
    
    for text in texts:
        reward = 0.0
        
        # é•¿åº¦å¥–åŠ± (é€‚ä¸­é•¿åº¦)
        length = len(text)
        if 20 <= length <= 100:
            reward += 1.0
        elif length > 100:
            reward += 0.5
        
        # ç§¯æè¯æ±‡å¥–åŠ±
        for word in positive_words:
            if word in text.lower():
                reward += 0.5
        
        rewards.append(reward)
    
    return rewards

def get_optimal_device():
    """è·å–æœ€ä¼˜è®¡ç®—è®¾å¤‡"""
    if torch.backends.mps.is_available():
        print("ğŸ ä½¿ç”¨è‹¹æœç¥ç»å¼•æ“(MPS)åŠ é€Ÿ")
        return "mps"
    elif torch.cuda.is_available():
        print("ğŸš€ ä½¿ç”¨CUDA GPUåŠ é€Ÿ")
        return "cuda"
    else:
        print("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼")
        return "cpu"

def get_device_optimized_config():
    """æ ¹æ®è®¾å¤‡è·å–ä¼˜åŒ–é…ç½®"""
    device = get_optimal_device()
    
    # æ£€æµ‹å†…å­˜å¤§å°
    import psutil
    memory_gb = psutil.virtual_memory().total / (1024**3)
    
    # æ ¹æ®è®¾å¤‡å’Œå†…å­˜è°ƒæ•´é…ç½®
    if device == "mps":
        # è‹¹æœèŠ¯ç‰‡ä¼˜åŒ–é…ç½®
        if memory_gb >= 16:
            return {
                "batch_size": 4,
                "mini_batch_size": 2,
                "gradient_accumulation_steps": 2,
                "optimize_cuda_cache": False,  # å…³é—­CUDAä¼˜åŒ–
            }
        else:
            return {
                "batch_size": 2,
                "mini_batch_size": 1,
                "gradient_accumulation_steps": 4,
                "optimize_cuda_cache": False,
            }
    elif device == "cuda":
        # GPUé…ç½®
        return {
            "batch_size": 8,
            "mini_batch_size": 4,
            "gradient_accumulation_steps": 1,
            "optimize_cuda_cache": True,
        }
    else:
        # CPUé…ç½®
        return {
            "batch_size": 2,
            "mini_batch_size": 1,
            "gradient_accumulation_steps": 4,
            "optimize_cuda_cache": False,
        }

def main():
    print("ğŸ¯ å¼€å§‹ç®€å•PPOè®­ç»ƒç¤ºä¾‹")
    
    # è·å–è®¾å¤‡ä¼˜åŒ–é…ç½®
    device_config = get_device_optimized_config()
    
    # 1. é…ç½®
    config = PPOConfig(
        learning_rate=1.41e-5,
        early_stopping=False,
        target_kl=0.1,
        ppo_epochs=4,
        seed=42,
        steps=20,  # å°‘é‡æ­¥éª¤ç”¨äºæ¼”ç¤º
        **device_config  # åº”ç”¨è®¾å¤‡ä¼˜åŒ–é…ç½®
    )
    
    # 2. åŠ è½½æ¨¡å‹å’Œtokenizer
    print("ğŸ“š åŠ è½½æ¨¡å‹å’Œtokenizer...")
    model = AutoModelForCausalLMWithValueHead.from_pretrained("gpt2")
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    
    # è®¾ç½®pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = tokenizer.pad_token_id
    
    # ç§»åŠ¨æ¨¡å‹åˆ°æœ€ä¼˜è®¾å¤‡
    device = get_optimal_device()
    try:
        model = model.to(device)
        print(f"ğŸ“± æ¨¡å‹å·²ç§»åŠ¨åˆ° {device} è®¾å¤‡")
    except Exception as e:
        print(f"âš ï¸  è®¾å¤‡ç§»åŠ¨å¤±è´¥ï¼Œä½¿ç”¨CPU: {e}")
        device = "cpu"
        model = model.to(device)
    
    # 3. å‡†å¤‡æ•°æ®
    print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
    dataset = create_sample_dataset()
    
    def tokenize_function(examples):
        return tokenizer(examples["query"], 
                        truncation=True, 
                        padding="max_length", 
                        max_length=64,
                        return_tensors="pt")
    
    # 4. åˆ›å»ºPPOè®­ç»ƒå™¨
    print("ğŸ‹ï¸ åˆ›å»ºPPOè®­ç»ƒå™¨...")
    ppo_trainer = PPOTrainer(
        config=config,
        model=model,
        ref_model=None,  # ä½¿ç”¨é»˜è®¤å‚è€ƒæ¨¡å‹
        tokenizer=tokenizer,
        dataset=dataset,
        data_collator=None,
    )
    
    print("ğŸ® å¼€å§‹è®­ç»ƒ...")
    
    # 5. è®­ç»ƒå¾ªç¯
    for epoch, batch in enumerate(ppo_trainer.dataloader):
        if epoch >= config.steps:
            break
            
        print(f"\n--- Epoch {epoch + 1} ---")
        
        # ç”Ÿæˆå›å¤
        query_tensors = batch["input_ids"]
        response_tensors = ppo_trainer.generate(
            query_tensors,
            return_prompt=False,
            max_length=128,
            do_sample=True,
            top_p=0.9,
            temperature=0.7,
        )
        
        # è§£ç æ–‡æœ¬ç”¨äºè®¡ç®—å¥–åŠ±
        batch_texts = []
        for i in range(len(query_tensors)):
            query_text = tokenizer.decode(query_tensors[i], skip_special_tokens=True)
            response_text = tokenizer.decode(response_tensors[i], skip_special_tokens=True)
            full_text = query_text + response_text
            batch_texts.append(response_text)
            print(f"Query: {query_text}")
            print(f"Response: {response_text}")
        
        # è®¡ç®—å¥–åŠ±
        rewards = simple_reward_function(batch_texts)
        rewards = [torch.tensor(r) for r in rewards]
        
        print(f"Rewards: {[r.item() for r in rewards]}")
        
        # PPOæ›´æ–°
        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        if stats:
            print(f"PPO Loss: {stats.get('ppo/loss/policy', 'N/A')}")
            print(f"Mean Reward: {stats.get('ppo/mean_scores', 'N/A')}")
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
    
    # 6. ä¿å­˜æ¨¡å‹
    model.save_pretrained("./models/ppo_gpt2_simple")
    tokenizer.save_pretrained("./models/ppo_gpt2_simple")
    print("ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ° ./models/ppo_gpt2_simple")

if __name__ == "__main__":
    main()