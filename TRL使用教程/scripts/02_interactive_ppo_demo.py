#!/usr/bin/env python3
"""
äº¤äº’å¼PPOæ¼”ç¤º
è®©ä½ ç›´æ¥ä½“éªŒPPOè®­ç»ƒå‰åçš„æ¨¡å‹å·®å¼‚
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from trl import AutoModelForCausalLMWithValueHead

def load_original_model():
    """åŠ è½½åŸå§‹GPT2æ¨¡å‹"""
    print("ğŸ“š åŠ è½½åŸå§‹GPT2æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    model = AutoModelForCausalLM.from_pretrained("gpt2")
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer

def load_trained_model():
    """åŠ è½½PPOè®­ç»ƒåçš„æ¨¡å‹"""
    try:
        print("ğŸ¯ åŠ è½½PPOè®­ç»ƒåçš„æ¨¡å‹...")
        tokenizer = AutoTokenizer.from_pretrained("./models/ppo_gpt2_simple")
        model = AutoModelForCausalLMWithValueHead.from_pretrained("./models/ppo_gpt2_simple")
        return model, tokenizer
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½è®­ç»ƒåçš„æ¨¡å‹: {e}")
        print("è¯·å…ˆè¿è¡Œ 01_simple_ppo_example.py è®­ç»ƒæ¨¡å‹")
        return None, None

def generate_text(model, tokenizer, prompt, max_length=100):
    """ç”Ÿæˆæ–‡æœ¬"""
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=max_length,
            do_sample=True,
            top_p=0.9,
            temperature=0.7,
            pad_token_id=tokenizer.pad_token_id
        )
    
    # å¦‚æœæ˜¯å¸¦value headçš„æ¨¡å‹ï¼Œåªå–logitséƒ¨åˆ†
    if hasattr(model, 'pretrained_model'):
        # å¯¹äºå¸¦value headçš„æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨generateçš„è¾“å‡º
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    else:
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return generated_text

def compare_models():
    """æ¯”è¾ƒè®­ç»ƒå‰åçš„æ¨¡å‹è¾“å‡º"""
    
    # åŠ è½½æ¨¡å‹
    original_model, original_tokenizer = load_original_model()
    trained_model, trained_tokenizer = load_trained_model()
    
    if trained_model is None:
        return
    
    test_prompts = [
        "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ï¼š",
        "ç»™æˆ‘ä¸€ä¸ªå¥åº·é¥®é£Ÿå»ºè®®ï¼š",
        "æ¨èä¸€æœ¬å¥½ä¹¦ï¼š",
        "æè¿°ä½ çš„ç†æƒ³å‡æœŸï¼š",
        "è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼š"
    ]
    
    print("\n" + "="*60)
    print("ğŸ” æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("="*60)
    
    for i, prompt in enumerate(test_prompts):
        print(f"\nğŸ¯ æµ‹è¯• {i+1}: {prompt}")
        print("-" * 50)
        
        # åŸå§‹æ¨¡å‹è¾“å‡º
        print("ğŸ“– åŸå§‹GPT2è¾“å‡º:")
        original_output = generate_text(original_model, original_tokenizer, prompt)
        print(original_output[len(prompt):])  # åªæ˜¾ç¤ºç”Ÿæˆçš„éƒ¨åˆ†
        
        print("\nğŸ¯ PPOè®­ç»ƒåè¾“å‡º:")
        trained_output = generate_text(trained_model, trained_tokenizer, prompt)
        print(trained_output[len(prompt):])  # åªæ˜¾ç¤ºç”Ÿæˆçš„éƒ¨åˆ†
        
        print("-" * 50)

def interactive_test():
    """äº¤äº’å¼æµ‹è¯•"""
    trained_model, trained_tokenizer = load_trained_model()
    
    if trained_model is None:
        return
    
    print("\nğŸ® äº¤äº’å¼æµ‹è¯•æ¨¡å¼")
    print("è¾“å…¥æç¤ºè¯ï¼Œæ¨¡å‹ä¼šç”Ÿæˆå›å¤ (è¾“å…¥ 'quit' é€€å‡º)")
    
    while True:
        prompt = input("\nğŸ’­ ä½ çš„æç¤º: ")
        if prompt.lower() == 'quit':
            break
        
        output = generate_text(trained_model, trained_tokenizer, prompt)
        print(f"ğŸ¤– æ¨¡å‹å›å¤: {output[len(prompt):]}")

def main():
    print("ğŸ¯ PPOæ¨¡å‹æ¼”ç¤ºç¨‹åº")
    
    while True:
        print("\né€‰æ‹©æ“ä½œ:")
        print("1. æ¯”è¾ƒè®­ç»ƒå‰åæ¨¡å‹")
        print("2. äº¤äº’å¼æµ‹è¯•")
        print("3. é€€å‡º")
        
        choice = input("è¯·é€‰æ‹© (1-3): ")
        
        if choice == "1":
            compare_models()
        elif choice == "2":
            interactive_test()
        elif choice == "3":
            print("ğŸ‘‹ å†è§ï¼")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

if __name__ == "__main__":
    main()