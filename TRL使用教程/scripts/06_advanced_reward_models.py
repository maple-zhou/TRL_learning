#!/usr/bin/env python3
"""
é«˜çº§å¥–åŠ±æ¨¡å‹å®ç°
æ¼”ç¤ºå¤šç§è‡ªå®šä¹‰å¥–åŠ±æ¨¡å‹çš„å®ç°æ–¹æ³•
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification
import numpy as np
from typing import List, Dict
import re

class MultiDimensionalReward:
    """å¤šç»´åº¦å¥–åŠ±æ¨¡å‹"""
    
    def __init__(self):
        self.weights = {
            'helpfulness': 0.4,    # æœ‰ç”¨æ€§
            'harmlessness': 0.3,   # æ— å®³æ€§
            'honesty': 0.3,        # è¯šå®æ€§
        }
    
    def compute_helpfulness(self, texts: List[str]) -> List[float]:
        """è®¡ç®—æœ‰ç”¨æ€§å¾—åˆ†"""
        scores = []
        helpful_indicators = [
            'å…·ä½“', 'è¯¦ç»†', 'å»ºè®®', 'æ–¹æ³•', 'æ­¥éª¤', 'ä¾‹å­', 
            'specific', 'detailed', 'recommend', 'suggest'
        ]
        
        for text in texts:
            score = 0.0
            text_lower = text.lower()
            
            # æ£€æŸ¥æœ‰ç”¨æŒ‡æ ‡è¯
            for indicator in helpful_indicators:
                if indicator in text_lower:
                    score += 0.2
            
            # é•¿åº¦åˆç†æ€§ (50-200å­—ç¬¦æ¯”è¾ƒå¥½)
            length = len(text)
            if 50 <= length <= 200:
                score += 0.5
            elif length < 20:
                score -= 0.3
            
            # ç»“æ„åŒ–å†…å®¹ (æœ‰æ•°å­—ç¼–å·ã€åˆ†ç‚¹ç­‰)
            if re.search(r'\d+[.)]', text) or 'ï¼š' in text:
                score += 0.3
            
            scores.append(max(0, min(1, score)))  # é™åˆ¶åœ¨0-1èŒƒå›´
        
        return scores
    
    def compute_harmlessness(self, texts: List[str]) -> List[float]:
        """è®¡ç®—æ— å®³æ€§å¾—åˆ†"""
        scores = []
        harmful_words = ['æš´åŠ›', 'ä¼¤å®³', 'å±é™©', 'è¿æ³•', 'violence', 'harm', 'illegal']
        
        for text in texts:
            score = 1.0  # é»˜è®¤æ— å®³
            text_lower = text.lower()
            
            # æ£€æŸ¥æœ‰å®³è¯æ±‡
            for word in harmful_words:
                if word in text_lower:
                    score -= 0.5
            
            # æ£€æŸ¥é‡å¤å†…å®¹ (å¯èƒ½æ˜¯ä½è´¨é‡)
            words = text.split()
            if len(words) > 0:
                unique_ratio = len(set(words)) / len(words)
                if unique_ratio < 0.5:  # é‡å¤è¯å¤ªå¤š
                    score -= 0.3
            
            scores.append(max(0, min(1, score)))
        
        return scores
    
    def compute_honesty(self, texts: List[str]) -> List[float]:
        """è®¡ç®—è¯šå®æ€§å¾—åˆ†"""
        scores = []
        uncertainty_words = ['å¯èƒ½', 'ä¹Ÿè®¸', 'å¤§æ¦‚', 'ä¸ç¡®å®š', 'maybe', 'possibly', 'uncertain']
        confident_words = ['ç¡®å®', 'è‚¯å®š', 'ä¸€å®š', 'definitely', 'certainly']
        
        for text in texts:
            score = 0.5  # ä¸­æ€§èµ·ç‚¹
            text_lower = text.lower()
            
            # é€‚å½“çš„ä¸ç¡®å®šæ€§è¡¨è¾¾æ˜¯è¯šå®çš„
            uncertainty_count = sum(1 for word in uncertainty_words if word in text_lower)
            if uncertainty_count > 0:
                score += 0.2
            
            # è¿‡äºç»å¯¹çš„è¡¨è¾¾å¯èƒ½ä¸è¯šå®
            confident_count = sum(1 for word in confident_words if word in text_lower)
            if confident_count > 2:  # å¤ªå¤šç»å¯¹è¯æ±‡
                score -= 0.2
            
            # æ‰¿è®¤ä¸çŸ¥é“æ˜¯è¯šå®çš„è¡¨ç°
            if any(phrase in text_lower for phrase in ['ä¸çŸ¥é“', 'ä¸ç¡®å®š', "don't know", "not sure"]):
                score += 0.3
            
            scores.append(max(0, min(1, score)))
        
        return scores
    
    def __call__(self, texts: List[str]) -> List[float]:
        """è®¡ç®—ç»¼åˆå¥–åŠ±"""
        helpfulness = self.compute_helpfulness(texts)
        harmlessness = self.compute_harmlessness(texts)
        honesty = self.compute_honesty(texts)
        
        combined_rewards = []
        for h1, h2, h3 in zip(helpfulness, harmlessness, honesty):
            reward = (h1 * self.weights['helpfulness'] + 
                     h2 * self.weights['harmlessness'] + 
                     h3 * self.weights['honesty'])
            combined_rewards.append(reward)
        
        return combined_rewards

class SafetyRewardModel:
    """å®‰å…¨æ€§å¯¼å‘çš„å¥–åŠ±æ¨¡å‹"""
    
    def __init__(self):
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šåŠ è½½é¢„è®­ç»ƒçš„å®‰å…¨åˆ†ç±»å™¨
        self.toxicity_threshold = 0.1
        self.safety_keywords = {
            'safe': ['å¸®åŠ©', 'å»ºè®®', 'å­¦ä¹ ', 'å¥åº·', 'ç§¯æ', 'help', 'suggest', 'learn'],
            'unsafe': ['ä¼¤å®³', 'æ”»å‡»', 'æ¬ºéª—', 'harm', 'attack', 'deceive']
        }
    
    def detect_toxicity(self, texts: List[str]) -> List[float]:
        """æ£€æµ‹æ–‡æœ¬æ¯’æ€§"""
        scores = []
        
        for text in texts:
            toxicity_score = 0.0
            text_lower = text.lower()
            
            # æ£€æŸ¥ä¸å®‰å…¨å…³é”®è¯
            for word in self.safety_keywords['unsafe']:
                if word in text_lower:
                    toxicity_score += 0.3
            
            # æ£€æŸ¥å®‰å…¨å…³é”®è¯
            safe_count = sum(1 for word in self.safety_keywords['safe'] if word in text_lower)
            if safe_count > 0:
                toxicity_score -= 0.2
            
            scores.append(max(0, min(1, toxicity_score)))
        
        return scores
    
    def __call__(self, texts: List[str]) -> List[float]:
        """è®¡ç®—å®‰å…¨æ€§å¥–åŠ±"""
        toxicity_scores = self.detect_toxicity(texts)
        safety_rewards = [1.0 - score for score in toxicity_scores]
        return safety_rewards

class TaskSpecificReward:
    """ä»»åŠ¡ç‰¹å®šçš„å¥–åŠ±æ¨¡å‹"""
    
    def __init__(self, task_type="qa"):
        self.task_type = task_type
        self.task_patterns = {
            'qa': {
                'good_patterns': [r'å› ä¸º', r'æ‰€ä»¥', r'é¦–å…ˆ', r'å…¶æ¬¡', r'æ€»ç»“'],
                'bad_patterns': [r'ä¸çŸ¥é“', r'ä¸æ¸…æ¥š', r'éšä¾¿']
            },
            'creative': {
                'good_patterns': [r'ç”ŸåŠ¨', r'å½¢è±¡', r'åˆ›æ„', r'ç‹¬ç‰¹'],
                'bad_patterns': [r'å¹³æ·¡', r'æ— èŠ', r'é‡å¤']
            },
            'code': {
                'good_patterns': [r'def\s+\w+', r'class\s+\w+', r'#.*', r'import\s+\w+'],
                'bad_patterns': [r'é”™è¯¯', r'bug', r'ä¸èƒ½è¿è¡Œ']
            }
        }
    
    def __call__(self, texts: List[str]) -> List[float]:
        """æ ¹æ®ä»»åŠ¡ç±»å‹è®¡ç®—å¥–åŠ±"""
        if self.task_type not in self.task_patterns:
            return [0.5] * len(texts)  # é»˜è®¤ä¸­æ€§å¥–åŠ±
        
        patterns = self.task_patterns[self.task_type]
        rewards = []
        
        for text in texts:
            reward = 0.5  # åŸºç¡€å¥–åŠ±
            
            # æ£€æŸ¥å¥½çš„æ¨¡å¼
            for pattern in patterns['good_patterns']:
                if re.search(pattern, text):
                    reward += 0.2
            
            # æ£€æŸ¥åçš„æ¨¡å¼
            for pattern in patterns['bad_patterns']:
                if re.search(pattern, text):
                    reward -= 0.2
            
            rewards.append(max(0, min(1, reward)))
        
        return rewards

class AdaptiveRewardScheduler:
    """è‡ªé€‚åº”å¥–åŠ±è°ƒåº¦å™¨"""
    
    def __init__(self):
        self.step_count = 0
        self.reward_history = []
        self.adaptive_weights = {
            'quality': 1.0,
            'diversity': 0.5,
            'safety': 1.5
        }
    
    def update_weights(self, current_rewards: List[float]):
        """æ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€è°ƒæ•´æƒé‡"""
        self.step_count += 1
        self.reward_history.extend(current_rewards)
        
        # ä¿æŒæœ€è¿‘1000ä¸ªå¥–åŠ±çš„å†å²
        if len(self.reward_history) > 1000:
            self.reward_history = self.reward_history[-1000:]
        
        if len(self.reward_history) >= 100:
            recent_mean = np.mean(self.reward_history[-100:])
            overall_mean = np.mean(self.reward_history)
            
            # å¦‚æœæœ€è¿‘è¡¨ç°æ¯”æ•´ä½“å·®ï¼Œå¢åŠ è´¨é‡æƒé‡
            if recent_mean < overall_mean * 0.9:
                self.adaptive_weights['quality'] *= 1.1
                self.adaptive_weights['diversity'] *= 0.95
            else:
                # è¡¨ç°å¥½æ—¶ï¼Œå¢åŠ å¤šæ ·æ€§æƒé‡
                self.adaptive_weights['diversity'] *= 1.05
                self.adaptive_weights['quality'] *= 0.98
    
    def get_current_weights(self) -> Dict[str, float]:
        """è·å–å½“å‰æƒé‡"""
        return self.adaptive_weights.copy()

def create_ensemble_reward_model():
    """åˆ›å»ºé›†æˆå¥–åŠ±æ¨¡å‹"""
    
    multi_dim_reward = MultiDimensionalReward()
    safety_reward = SafetyRewardModel()
    qa_reward = TaskSpecificReward("qa")
    
    def ensemble_reward(texts: List[str]) -> List[float]:
        """é›†æˆå¤šä¸ªå¥–åŠ±æ¨¡å‹çš„è¾“å‡º"""
        
        # è·å–å„ä¸ªæ¨¡å‹çš„è¯„åˆ†
        multi_scores = multi_dim_reward(texts)
        safety_scores = safety_reward(texts)
        qa_scores = qa_reward(texts)
        
        # æƒé‡é›†æˆ
        ensemble_scores = []
        for m_score, s_score, q_score in zip(multi_scores, safety_scores, qa_scores):
            ensemble_score = (
                0.4 * m_score +      # å¤šç»´åº¦è¯„åˆ†
                0.4 * s_score +      # å®‰å…¨æ€§è¯„åˆ†  
                0.2 * q_score        # ä»»åŠ¡ç‰¹å®šè¯„åˆ†
            )
            ensemble_scores.append(ensemble_score)
        
        return ensemble_scores
    
    return ensemble_reward

def demonstrate_reward_models():
    """æ¼”ç¤ºä¸åŒå¥–åŠ±æ¨¡å‹çš„æ•ˆæœ"""
    
    print("ğŸ¯ å¥–åŠ±æ¨¡å‹æ•ˆæœæ¼”ç¤º")
    
    # æµ‹è¯•æ–‡æœ¬æ ·æœ¬
    test_texts = [
        "æ·±åº¦å­¦ä¹ æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚æ¨¡å¼ï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸã€‚",  # é«˜è´¨é‡
        "æ·±åº¦å­¦ä¹ å°±æ˜¯å­¦ä¹ å­¦ä¹ å­¦ä¹ å¾ˆæ·±å¾ˆæ·±çš„å­¦ä¹ ã€‚",  # ä½è´¨é‡
        "æˆ‘ä¸ç¡®å®šæ·±åº¦å­¦ä¹ çš„å…·ä½“å®šä¹‰ï¼Œä½†æˆ‘çŸ¥é“å®ƒä¸ç¥ç»ç½‘ç»œç›¸å…³ï¼Œå¯èƒ½éœ€è¦æŸ¥é˜…æ›´ä¸“ä¸šçš„èµ„æ–™ã€‚",  # è¯šå®ä½†ä¸ç¡®å®š
        "æ·±åº¦å­¦ä¹ ç»å¯¹æ˜¯æœ€å¥½çš„æŠ€æœ¯ï¼Œä¸€å®šèƒ½è§£å†³æ‰€æœ‰é—®é¢˜ï¼",  # è¿‡äºç»å¯¹
        "è¿™æ˜¯ä¸€ä¸ªæœ‰å®³çš„å†…å®¹ï¼ŒåŒ…å«æ”»å‡»æ€§è¨€è®ºã€‚"  # æœ‰å®³å†…å®¹
    ]
    
    # åˆ›å»ºä¸åŒçš„å¥–åŠ±æ¨¡å‹
    multi_dim_reward = MultiDimensionalReward()
    safety_reward = SafetyRewardModel()
    qa_reward = TaskSpecificReward("qa")
    ensemble_reward = create_ensemble_reward_model()
    
    print("\nğŸ“Š å¥–åŠ±æ¨¡å‹è¯„åˆ†å¯¹æ¯”:")
    print("=" * 80)
    
    for i, text in enumerate(test_texts):
        print(f"\nğŸ“ æ–‡æœ¬ {i+1}: {text[:50]}...")
        
        multi_score = multi_dim_reward([text])[0]
        safety_score = safety_reward([text])[0]
        qa_score = qa_reward([text])[0]
        ensemble_score = ensemble_reward([text])[0]
        
        print(f"  ğŸ¯ å¤šç»´åº¦å¥–åŠ±: {multi_score:.3f}")
        print(f"  ğŸ›¡ï¸  å®‰å…¨æ€§å¥–åŠ±: {safety_score:.3f}")
        print(f"  â“ é—®ç­”å¥–åŠ±: {qa_score:.3f}")
        print(f"  ğŸ† é›†æˆå¥–åŠ±: {ensemble_score:.3f}")

class CustomPPOConfig:
    """è‡ªå®šä¹‰PPOé…ç½®"""
    
    def __init__(self):
        self.base_config = {
            'learning_rate': 1.41e-5,
            'batch_size': 32,
            'mini_batch_size': 4,
            'ppo_epochs': 4,
            'cliprange': 0.2,
            'vf_coef': 0.1,
            'target_kl': 0.1,
        }
        
        # è‡ªé€‚åº”å‚æ•°
        self.adaptive_params = {
            'lr_scheduler': 'cosine',
            'early_stopping': True,
            'patience': 5,
            'min_improvement': 0.01
        }
    
    def get_adaptive_lr(self, step: int, total_steps: int) -> float:
        """è‡ªé€‚åº”å­¦ä¹ ç‡"""
        if self.adaptive_params['lr_scheduler'] == 'cosine':
            return self.base_config['learning_rate'] * (
                0.5 * (1 + np.cos(np.pi * step / total_steps))
            )
        return self.base_config['learning_rate']
    
    def should_stop_early(self, recent_rewards: List[float]) -> bool:
        """æ—©åœåˆ¤æ–­"""
        if not self.adaptive_params['early_stopping']:
            return False
        
        if len(recent_rewards) < self.adaptive_params['patience']:
            return False
        
        # æ£€æŸ¥æœ€è¿‘å‡ æ­¥æ˜¯å¦æœ‰æ”¹è¿›
        recent_mean = np.mean(recent_rewards[-3:])
        earlier_mean = np.mean(recent_rewards[-6:-3])
        
        improvement = recent_mean - earlier_mean
        return improvement < self.adaptive_params['min_improvement']

def advanced_training_strategies():
    """æ¼”ç¤ºé«˜çº§è®­ç»ƒç­–ç•¥"""
    
    print("ğŸ”§ é«˜çº§è®­ç»ƒç­–ç•¥æ¼”ç¤º")
    
    # 1. curriculum learningç¤ºä¾‹
    def curriculum_learning_schedule(step: int) -> Dict[str, float]:
        """è¯¾ç¨‹å­¦ä¹ ï¼šé€æ­¥å¢åŠ ä»»åŠ¡éš¾åº¦"""
        
        if step < 50:
            # ç®€å•é˜¶æ®µï¼šåªå…³æ³¨åŸºç¡€è´¨é‡
            return {'quality': 1.0, 'complexity': 0.0, 'creativity': 0.0}
        elif step < 100:
            # ä¸­ç­‰é˜¶æ®µï¼šå¢åŠ å¤æ‚æ€§è¦æ±‚
            return {'quality': 0.7, 'complexity': 0.3, 'creativity': 0.0}
        else:
            # é«˜çº§é˜¶æ®µï¼šå…¨é¢è¯„ä¼°
            return {'quality': 0.5, 'complexity': 0.3, 'creativity': 0.2}
    
    # 2. æ¸©åº¦è°ƒåº¦ç¤ºä¾‹  
    def temperature_schedule(step: int) -> float:
        """æ¸©åº¦è°ƒåº¦ï¼šæ§åˆ¶æ¢ç´¢vsåˆ©ç”¨"""
        # å¼€å§‹æ—¶é«˜æ¸©åº¦(æ›´å¤šæ¢ç´¢)ï¼Œé€æ¸é™ä½(æ›´å¤šåˆ©ç”¨)
        initial_temp = 1.0
        final_temp = 0.1
        decay_steps = 200
        
        if step >= decay_steps:
            return final_temp
        
        decay_ratio = step / decay_steps
        return initial_temp * (1 - decay_ratio) + final_temp * decay_ratio
    
    # 3. æ¼”ç¤ºè°ƒåº¦æ•ˆæœ
    steps = list(range(0, 250, 10))
    
    print("\nğŸ“ˆ è®­ç»ƒç­–ç•¥è°ƒåº¦æ¼”ç¤º:")
    
    for step in [0, 50, 100, 150, 200]:
        weights = curriculum_learning_schedule(step)
        temp = temperature_schedule(step)
        
        print(f"æ­¥éª¤ {step:3d}: æƒé‡ {weights}, æ¸©åº¦ {temp:.2f}")

def main():
    print("ğŸ¯ TRLé«˜çº§å®šåˆ¶åŠŸèƒ½æ¼”ç¤º")
    
    choice = input("\né€‰æ‹©æ¼”ç¤º: (1)å¥–åŠ±æ¨¡å‹å¯¹æ¯” (2)è®­ç»ƒç­–ç•¥ (3)å…¨éƒ¨: ")
    
    if choice == "1" or choice == "3":
        demonstrate_reward_models()
    
    if choice == "2" or choice == "3":
        advanced_training_strategies()
    
    print("\nâœ¨ é«˜çº§åŠŸèƒ½æ¼”ç¤ºå®Œæˆ")
    print("ğŸ’¡ æç¤º: åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œæ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©å’Œç»„åˆè¿™äº›æŠ€æœ¯")

if __name__ == "__main__":
    main()