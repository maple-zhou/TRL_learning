#!/usr/bin/env python3
"""
è‡ªå®šä¹‰è®­ç»ƒç­–ç•¥å®ç°
æ¼”ç¤ºå¦‚ä½•å®ç°å„ç§é«˜çº§è®­ç»ƒæŠ€å·§å’Œç­–ç•¥
"""

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModelForCausalLM
from trl import PPOConfig, PPOTrainer, AutoModelForCausalLMWithValueHead
from datasets import Dataset
import numpy as np
import matplotlib.pyplot as plt
from typing import List, Dict, Optional
import math

class CurriculumLearning:
    """è¯¾ç¨‹å­¦ä¹ å®ç°"""
    
    def __init__(self):
        self.current_stage = 0
        self.stage_thresholds = [0.3, 0.6, 0.8]  # è¿›å…¥ä¸‹ä¸€é˜¶æ®µçš„å¥–åŠ±é˜ˆå€¼
        
        self.stages = [
            {
                'name': 'åŸºç¡€é˜¶æ®µ',
                'description': 'å­¦ä¹ åŸºæœ¬çš„å›å¤æ ¼å¼å’Œç¤¼è²Œç”¨è¯­',
                'reward_weights': {'politeness': 0.8, 'length': 0.2, 'coherence': 0.0},
                'data_complexity': 'simple'
            },
            {
                'name': 'ä¸­çº§é˜¶æ®µ', 
                'description': 'æé«˜å›å¤çš„è¿è´¯æ€§å’Œä¿¡æ¯å«é‡',
                'reward_weights': {'politeness': 0.3, 'length': 0.2, 'coherence': 0.5},
                'data_complexity': 'medium'
            },
            {
                'name': 'é«˜çº§é˜¶æ®µ',
                'description': 'ç”Ÿæˆåˆ›é€ æ€§å’Œæ·±åº¦çš„å›å¤',
                'reward_weights': {'politeness': 0.2, 'length': 0.1, 'coherence': 0.3, 'creativity': 0.4},
                'data_complexity': 'complex'
            }
        ]
    
    def should_advance_stage(self, recent_rewards: List[float]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µ"""
        if self.current_stage >= len(self.stage_thresholds):
            return False
        
        if len(recent_rewards) < 10:
            return False
        
        avg_reward = np.mean(recent_rewards[-10:])
        threshold = self.stage_thresholds[self.current_stage]
        
        return avg_reward >= threshold
    
    def advance_stage(self):
        """è¿›å…¥ä¸‹ä¸€é˜¶æ®µ"""
        if self.current_stage < len(self.stages) - 1:
            self.current_stage += 1
            print(f"ğŸ“ è¿›å…¥{self.stages[self.current_stage]['name']}")
            print(f"   {self.stages[self.current_stage]['description']}")
    
    def get_current_stage_info(self) -> Dict:
        """è·å–å½“å‰é˜¶æ®µä¿¡æ¯"""
        return self.stages[self.current_stage]
    
    def compute_stage_reward(self, texts: List[str]) -> List[float]:
        """æ ¹æ®å½“å‰é˜¶æ®µè®¡ç®—å¥–åŠ±"""
        stage_info = self.get_current_stage_info()
        weights = stage_info['reward_weights']
        
        # è®¡ç®—å„ä¸ªç»´åº¦çš„å¾—åˆ†
        politeness_scores = self._compute_politeness(texts)
        length_scores = self._compute_length_score(texts)
        coherence_scores = self._compute_coherence(texts)
        creativity_scores = self._compute_creativity(texts) if 'creativity' in weights else [0] * len(texts)
        
        # åŠ æƒå¹³å‡
        final_rewards = []
        for p, l, c, cr in zip(politeness_scores, length_scores, coherence_scores, creativity_scores):
            reward = (
                weights.get('politeness', 0) * p +
                weights.get('length', 0) * l +
                weights.get('coherence', 0) * c +
                weights.get('creativity', 0) * cr
            )
            final_rewards.append(reward)
        
        return final_rewards
    
    def _compute_politeness(self, texts: List[str]) -> List[float]:
        """è®¡ç®—ç¤¼è²Œç¨‹åº¦"""
        polite_words = ['è¯·', 'è°¢è°¢', 'æ‚¨å¥½', 'ä¸å¥½æ„æ€', 'please', 'thank', 'sorry']
        scores = []
        
        for text in texts:
            score = 0.5  # åŸºç¡€åˆ†
            text_lower = text.lower()
            
            for word in polite_words:
                if word in text_lower:
                    score += 0.1
            
            scores.append(min(1.0, score))
        
        return scores
    
    def _compute_length_score(self, texts: List[str]) -> List[float]:
        """è®¡ç®—é•¿åº¦åˆç†æ€§"""
        scores = []
        
        for text in texts:
            length = len(text)
            
            if 20 <= length <= 100:
                score = 1.0
            elif 10 <= length < 20 or 100 < length <= 150:
                score = 0.7
            elif length < 10:
                score = 0.3
            else:
                score = 0.5
            
            scores.append(score)
        
        return scores
    
    def _compute_coherence(self, texts: List[str]) -> List[float]:
        """è®¡ç®—è¿è´¯æ€§"""
        scores = []
        
        for text in texts:
            # ç®€å•çš„è¿è´¯æ€§æ£€æŸ¥
            sentences = text.split('ã€‚')
            
            if len(sentences) <= 1:
                score = 0.5
            else:
                # æ£€æŸ¥å¥å­ä¹‹é—´çš„è¿æ¥è¯
                connectors = ['å› æ­¤', 'æ‰€ä»¥', 'ç„¶è€Œ', 'ä½†æ˜¯', 'å¹¶ä¸”', 'è€Œä¸”']
                connector_count = sum(1 for conn in connectors if conn in text)
                score = min(1.0, 0.3 + connector_count * 0.2)
            
            scores.append(score)
        
        return scores
    
    def _compute_creativity(self, texts: List[str]) -> List[float]:
        """è®¡ç®—åˆ›é€ æ€§"""
        creative_words = ['åˆ›æ–°', 'ç‹¬ç‰¹', 'æ–°é¢–', 'æƒ³è±¡', 'creative', 'innovative', 'unique']
        scores = []
        
        for text in texts:
            score = 0.3  # åŸºç¡€åˆ›é€ æ€§åˆ†æ•°
            text_lower = text.lower()
            
            # æ£€æŸ¥åˆ›é€ æ€§è¯æ±‡
            for word in creative_words:
                if word in text_lower:
                    score += 0.2
            
            # æ£€æŸ¥è¯æ±‡å¤šæ ·æ€§
            words = text.split()
            if len(words) > 0:
                unique_ratio = len(set(words)) / len(words)
                score += unique_ratio * 0.3
            
            scores.append(min(1.0, score))
        
        return scores

class OnlineRewardLearning:
    """åœ¨çº¿å¥–åŠ±å­¦ä¹ """
    
    def __init__(self):
        self.reward_buffer = []
        self.quality_buffer = []
        self.learning_rate = 0.01
        
        # å¯å­¦ä¹ çš„å¥–åŠ±æƒé‡
        self.learned_weights = {
            'length': 0.3,
            'sentiment': 0.3, 
            'coherence': 0.4
        }
    
    def update_reward_function(self, texts: List[str], human_scores: List[float]):
        """æ ¹æ®äººç±»åé¦ˆæ›´æ–°å¥–åŠ±å‡½æ•°"""
        
        # è®¡ç®—å½“å‰å¥–åŠ±å‡½æ•°çš„é¢„æµ‹
        predicted_scores = self.compute_learned_rewards(texts)
        
        # è®¡ç®—é¢„æµ‹è¯¯å·®
        errors = [h - p for h, p in zip(human_scores, predicted_scores)]
        
        # æ›´æ–°æƒé‡ï¼ˆç®€åŒ–çš„æ¢¯åº¦ä¸‹é™ï¼‰
        features = self._extract_features(texts)
        
        for i, error in enumerate(errors):
            for feature_name, feature_value in features[i].items():
                if feature_name in self.learned_weights:
                    self.learned_weights[feature_name] += self.learning_rate * error * feature_value
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(self.learned_weights.values())
        for key in self.learned_weights:
            self.learned_weights[key] /= total_weight
        
        print(f"ğŸ”„ å¥–åŠ±å‡½æ•°æƒé‡æ›´æ–°: {self.learned_weights}")
    
    def _extract_features(self, texts: List[str]) -> List[Dict[str, float]]:
        """æå–æ–‡æœ¬ç‰¹å¾"""
        features_list = []
        
        for text in texts:
            features = {
                'length': min(1.0, len(text) / 100),  # å½’ä¸€åŒ–é•¿åº¦
                'sentiment': self._simple_sentiment(text),
                'coherence': self._simple_coherence(text)
            }
            features_list.append(features)
        
        return features_list
    
    def _simple_sentiment(self, text: str) -> float:
        """ç®€å•æƒ…æ„Ÿåˆ†æ"""
        positive_words = ['å¥½', 'æ£’', 'ä¼˜ç§€', 'å–œæ¬¢', 'good', 'great', 'excellent']
        negative_words = ['å', 'å·®', 'ç³Ÿç³•', 'bad', 'terrible', 'awful']
        
        pos_count = sum(1 for word in positive_words if word in text.lower())
        neg_count = sum(1 for word in negative_words if word in text.lower())
        
        return (pos_count - neg_count + 1) / 2  # å½’ä¸€åŒ–åˆ°0-1
    
    def _simple_coherence(self, text: str) -> float:
        """ç®€å•è¿è´¯æ€§è¯„ä¼°"""
        sentences = text.split('ã€‚')
        if len(sentences) <= 1:
            return 0.5
        
        # æ£€æŸ¥è¿æ¥è¯
        connectors = ['å› æ­¤', 'æ‰€ä»¥', 'ç„¶è€Œ', 'ä½†æ˜¯']
        connector_count = sum(1 for conn in connectors if conn in text)
        return min(1.0, connector_count / len(sentences))
    
    def compute_learned_rewards(self, texts: List[str]) -> List[float]:
        """ä½¿ç”¨å­¦ä¹ åˆ°çš„æƒé‡è®¡ç®—å¥–åŠ±"""
        features_list = self._extract_features(texts)
        rewards = []
        
        for features in features_list:
            reward = sum(
                self.learned_weights[name] * value 
                for name, value in features.items()
                if name in self.learned_weights
            )
            rewards.append(reward)
        
        return rewards

def demonstrate_curriculum_learning():
    """æ¼”ç¤ºè¯¾ç¨‹å­¦ä¹ æ•ˆæœ"""
    
    print("ğŸ“ è¯¾ç¨‹å­¦ä¹ æ¼”ç¤º")
    
    curriculum = CurriculumLearning()
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    all_rewards = []
    stage_changes = []
    
    for step in range(100):
        # æ¨¡æ‹Ÿä¸€ä¸ªbatchçš„å¥–åŠ±
        batch_rewards = np.random.normal(0.4 + step * 0.003, 0.1, 5)
        batch_rewards = [max(0, min(1, r)) for r in batch_rewards]
        all_rewards.extend(batch_rewards)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
        if curriculum.should_advance_stage(all_rewards):
            stage_changes.append(step)
            curriculum.advance_stage()
    
    # ç»˜åˆ¶å­¦ä¹ æ›²çº¿
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    # è®¡ç®—ç§»åŠ¨å¹³å‡
    window_size = 10
    moving_avg = []
    for i in range(len(all_rewards)):
        start_idx = max(0, i - window_size + 1)
        moving_avg.append(np.mean(all_rewards[start_idx:i+1]))
    
    plt.plot(moving_avg)
    for change_step in stage_changes:
        plt.axvline(x=change_step*5, color='red', linestyle='--', alpha=0.7)
    plt.title('è¯¾ç¨‹å­¦ä¹ å¥–åŠ±æ›²çº¿')
    plt.xlabel('è®­ç»ƒæ­¥éª¤')
    plt.ylabel('å¹³å‡å¥–åŠ±')
    plt.grid(True, alpha=0.3)
    
    plt.subplot(1, 2, 2)
    stage_names = [stage['name'] for stage in curriculum.stages]
    stage_counts = [0] * len(stage_names)
    
    current_stage = 0
    for step in range(100):
        if step in [s//5 for s in stage_changes]:
            current_stage += 1
        if current_stage < len(stage_counts):
            stage_counts[current_stage] += 1
    
    plt.bar(stage_names, stage_counts)
    plt.title('å„é˜¶æ®µè®­ç»ƒæ­¥æ•°åˆ†å¸ƒ')
    plt.xticks(rotation=45)
    
    plt.tight_layout()
    plt.show()
    
    print("ğŸ“Š è¯¾ç¨‹å­¦ä¹ å¯è§†åŒ–å®Œæˆ")

def main():
    print("ğŸ¯ é«˜çº§è®­ç»ƒç­–ç•¥æ¼”ç¤º")
    
    while True:
        print("\né€‰æ‹©æ¼”ç¤º:")
        print("1. å¥–åŠ±æ¨¡å‹å¯¹æ¯”")
        print("2. è¯¾ç¨‹å­¦ä¹ ç­–ç•¥")
        print("3. åœ¨çº¿å¥–åŠ±å­¦ä¹ ")
        print("4. é€€å‡º")
        
        choice = input("è¯·é€‰æ‹© (1-4): ")
        
        if choice == "1":
            from scripts.advanced_reward_models import demonstrate_reward_models
            demonstrate_reward_models()
        elif choice == "2":
            demonstrate_curriculum_learning()
        elif choice == "3":
            online_learner = OnlineRewardLearning()
            
            # æ¼”ç¤ºåœ¨çº¿å­¦ä¹ 
            sample_texts = ["è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å›ç­”", "å›ç­”å›ç­”å›ç­”", "æˆ‘è®¤ä¸ºè¿™ä¸ªé—®é¢˜å¾ˆæœ‰è¶£"]
            human_feedback = [0.8, 0.2, 0.7]  # æ¨¡æ‹Ÿäººç±»è¯„åˆ†
            
            print("åˆå§‹æƒé‡:", online_learner.learned_weights)
            online_learner.update_reward_function(sample_texts, human_feedback)
            print("æ›´æ–°åæƒé‡:", online_learner.learned_weights)
            
        elif choice == "4":
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main()